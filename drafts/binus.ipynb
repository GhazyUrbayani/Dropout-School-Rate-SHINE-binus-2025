{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c0d6d7b"
      },
      "source": [
        "# --- Combined Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV # Added RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures # Added PolynomialFeatures\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import warnings\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error # Added evaluation metrics\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9b1e2d1"
      },
      "source": [
        "# --- Data Preparation Function ---\n",
        "def dataPrep():\n",
        "    try:\n",
        "        train_df = pd.read_csv('train_dataset.csv')\n",
        "        test_df = pd.read_csv('test_dataset.csv')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Pastikan file dataset ada.\")\n",
        "        return None, None # Return None, None in case of error\n",
        "    return train_df, test_df # Always return the dataframes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d401480c",
        "outputId": "784ffa4e-8bf8-4817-fbb8-2c7361e5b8d8"
      },
      "source": [
        "# --- Alternatif 1 Execution (Rewritten from zBfiVzrw-Cr4) ---\n",
        "# 1 Cell ini berisi 1 Alternatif\n",
        "# Note: Assumes necessary imports and dataPrep are run in previous cells\n",
        "\n",
        "# --- 1. Custom Transformer untuk Feature Engineering ---\n",
        "class FeatureEngineeringTransformer_Alt1(BaseEstimator, TransformerMixin): # Renamed to avoid conflict if original zBfiVzrw-Cr4 is also kept\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Contoh Fitur Interaksi dan Rasio\n",
        "        # Hindari pembagian dengan nol\n",
        "        df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "\n",
        "        # Ekstrak kata kunci dari nama sekolah (jika ada)\n",
        "        df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "# --- 2. Fungsi Utama untuk Menjalankan Proses ---\n",
        "def run_training_pipeline_alt1_rewritten(): # Renamed function\n",
        "    # --- Memuat Data ---\n",
        "    # Using dataPrep function from separate cell\n",
        "    train_df, test_df = dataPrep()\n",
        "    if train_df is None or test_df is None:\n",
        "        print(\"Gagal memuat data. Menghentikan eksekusi Alternatif 1.\")\n",
        "        return\n",
        "\n",
        "    # --- Persiapan Awal ---\n",
        "    train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "    X = train_df.drop('dropout_rate_percent', axis=1)\n",
        "    y = train_df['dropout_rate_percent']\n",
        "\n",
        "    # Simpan ID test untuk file submission\n",
        "    test_ids = test_df['id']\n",
        "\n",
        "    # --- Identifikasi Tipe Kolom ---\n",
        "    # `school_name` akan digunakan oleh transformer custom, sisanya akan diproses\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    if 'id' in numerical_features:\n",
        "        numerical_features.remove('id')\n",
        "    # school_name will be handled by the FeatureEngineeringTransformer and the categorical transformer\n",
        "    # so we remove it from the list of columns to be passed to the categorical transformer\n",
        "    # if 'school_name' in categorical_features:\n",
        "    #     categorical_features.remove('school_name')\n",
        "\n",
        "\n",
        "    # --- Membangun Pipeline Preprocessing ---\n",
        "    # Menggunakan KNNImputer untuk imputasi yang lebih cerdas\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=5)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # handle_unknown='ignore' sangat penting untuk menangani kategori di test set\n",
        "    # yang mungkin tidak ada di train set.\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Gabungkan semua transformer preprocessing menjadi satu objek\n",
        "    # Need to include the features created by FeatureEngineeringTransformer in the ColumnTransformer's list of features to process.\n",
        "    # The FeatureEngineeringTransformer adds 'is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction'\n",
        "    # These should be treated as numerical features after creation.\n",
        "    # The school_name column itself needs to be passed through for the FE transformer.\n",
        "    # The original Alt1 preprocessor included 'school_name' in categorical_features.\n",
        "    # Let's replicate that logic here.\n",
        "    all_features_after_fe = numerical_features + categorical_features + ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction'] # Include new FE features\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, [f for f in numerical_features + ['funding_per_teacher', 'low_income_minority_interaction'] if f in all_features_after_fe]), # Apply to original and new numerical-like\n",
        "            ('cat', categorical_transformer, [f for f in categorical_features + ['is_high_school', 'is_middle_school', 'is_elementary_school'] if f in all_features_after_fe]) # Apply to original and new categorical-like\n",
        "        ],\n",
        "        remainder='passthrough' # This is crucial to pass through features not explicitly handled, like 'school_name' before FE\n",
        "    )\n",
        "\n",
        "    # --- Membangun Pipeline Model Lengkap ---\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('feature_engineering', FeatureEngineeringTransformer_Alt1()), # Use renamed transformer\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', lgb.LGBMRegressor(random_state=42))\n",
        "    ])\n",
        "\n",
        "    # --- Hyperparameter Tuning dengan GridSearchCV ---\n",
        "    # Ini adalah pencarian parameter terbaik untuk model\n",
        "    # Grid ini kecil untuk kecepatan, bisa diperluas untuk akurasi lebih tinggi\n",
        "    param_grid = {\n",
        "        'regressor__n_estimators': [100, 200],\n",
        "        'regressor__learning_rate': [0.05, 0.1],\n",
        "        'regressor__num_leaves': [31, 50],\n",
        "        'regressor__max_depth': [-1, 10]\n",
        "    }\n",
        "\n",
        "    # Gunakan K-Fold Cross-Validation untuk evaluasi yang robust\n",
        "    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    print(\"Memulai pencarian hyperparameter terbaik untuk Alternatif 1...\")\n",
        "    grid_search = GridSearchCV(model_pipeline, param_grid, cv=cv_strategy,\n",
        "                               scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    print(f\"Hyperparameter terbaik ditemukan untuk Alternatif 1: {grid_search.best_params_}\")\n",
        "    print(f\"Skor RMSE validasi silang terbaik untuk Alternatif 1: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # --- Finalisasi Model dan Prediksi ---\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Latih model terbaik pada SELURUH data training\n",
        "    best_model.fit(X, y)\n",
        "\n",
        "    # Simpan model yang sudah dilatih untuk digunakan nanti\n",
        "    # joblib.dump(best_model, 'best_dropout_prediction_model_alt1.pkl') # Optional: Save model\n",
        "    # print(\"Model terbaik Alternatif 1 telah disimpan sebagai 'best_dropout_prediction_model_alt1.pkl'\")\n",
        "\n",
        "    # Lakukan prediksi pada data test\n",
        "    test_predictions = best_model.predict(test_df)\n",
        "\n",
        "    # Buat file submission\n",
        "    submission_df = pd.DataFrame({'id': test_ids, 'dropout_rate_percent': test_predictions})\n",
        "    submission_df.to_csv('predictions_alt1.csv', index=False) # Save to unique file\n",
        "    print(\"File 'predictions_alt1.csv' berhasil dibuat.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training_pipeline_alt1_rewritten() # Call renamed function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai pencarian hyperparameter terbaik untuk Alternatif 1...\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1986\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 31\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Hyperparameter terbaik ditemukan untuk Alternatif 1: {'regressor__learning_rate': 0.05, 'regressor__max_depth': 10, 'regressor__n_estimators': 100, 'regressor__num_leaves': 31}\n",
            "Skor RMSE validasi silang terbaik untuk Alternatif 1: 4.4424\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1986\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 31\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "File 'predictions_alt1.csv' berhasil dibuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9f177b5",
        "outputId": "be25b491-ef9c-4cee-8897-43e9803502bc"
      },
      "source": [
        "# --- Combined Code for Alternatif 2 (Pipeline Definition and Execution) ---\n",
        "# Note: Assumes necessary imports and dataPrep are run in previous cells\n",
        "\n",
        "# Alternatif 2 (Pipeline Definition from aBmYonY0FNPv)\n",
        "class FeatureEngineeringTransformer_aBmYonY0FNPv(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer custom untuk membuat fitur baru secara terisolasi.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Mencegah pembagian dengan nol dengan menambahkan nilai kecil (epsilon)\n",
        "        epsilon = 1e-6\n",
        "\n",
        "        # Fitur Rasio dan Interaksi\n",
        "        df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        df['low_income_to_funding_ratio'] = df['percent_low_income'] / (df['funding_per_student_usd'] + epsilon)\n",
        "        df['minority_to_teacher_ratio'] = df['percent_minority'] / (df['student_teacher_ratio'] + epsilon)\n",
        "        df['test_score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + epsilon)\n",
        "\n",
        "        return df\n",
        "\n",
        "def create_full_pipeline_aBmYonY0FNPv(numerical_features, categorical_features):\n",
        "    # Pipeline untuk fitur numerik dasar\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=5)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Pipeline untuk membuat fitur polinomial dari fitur numerik yang sudah bersih\n",
        "    polynomial_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=5)),\n",
        "        ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Pipeline untuk fitur kategorikal\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            # Gunakan polynomial transformer pada subset fitur numerik yang paling mungkin berinteraksi\n",
        "            ('poly', polynomial_transformer, ['funding_per_student_usd', 'percent_low_income', 'student_teacher_ratio']),\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('feature_engineering', FeatureEngineeringTransformer_aBmYonY0FNPv()),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', lgb.LGBMRegressor(random_state=42))\n",
        "    ])\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "# Fungsi Tuning Hyperparameter dan Eksekusi untuk Alternatif 2\n",
        "def HyperparameterTune_alt2():\n",
        "    # -- Setup --\n",
        "    # Using dataPrep function from separate cell\n",
        "    train_df, test_df = dataPrep()\n",
        "    if train_df is None or test_df is None:\n",
        "        print(\"Gagal memuat data. Menghentikan eksekusi Alternatif 2.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "    X = train_df.drop('dropout_rate_percent', axis=1)\n",
        "    y = train_df['dropout_rate_percent']\n",
        "    test_ids = test_df['id']\n",
        "\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # Ensure this calls the correct pipeline function for Alternatif 2\n",
        "    pipeline = create_full_pipeline_aBmYonY0FNPv(numerical_features, categorical_features)\n",
        "\n",
        "    # Grid search yang lebih luas\n",
        "    param_grid = {\n",
        "        'regressor__n_estimators': [100, 200, 300],\n",
        "        'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'regressor__num_leaves': [20, 31, 40],\n",
        "        'regressor__colsample_bytree': [0.8, 0.9, 1.0] # Menambah variasi\n",
        "    }\n",
        "    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    print(\"Memulai pencarian hyperparameter yang disempurnakan untuk Alternatif 2...\")\n",
        "    # Menggunakan RandomizedSearchCV untuk efisiensi pada grid yang besar\n",
        "    # from sklearn.model_selection import RandomizedSearchCV # Already imported\n",
        "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=20,\n",
        "                                       cv=cv_strategy, scoring='neg_root_mean_squared_error',\n",
        "                                       n_jobs=-1, verbose=0, random_state=42)\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    best_model = random_search.best_estimator_\n",
        "    print(f\"Hyperparameter terbaik ditemukan untuk Alternatif 2: {random_search.best_params_}\")\n",
        "    print(f\"Skor RMSE validasi silang terbaik untuk Alternatif 2: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    return best_model, X, y, test_df, test_ids\n",
        "\n",
        "def Execution_alt2():\n",
        "    best_model, X, y, test_df, test_ids = HyperparameterTune_alt2()\n",
        "    # Check if HyperparameterTune_alt2 returned None due to data loading error\n",
        "    if best_model is None:\n",
        "        return\n",
        "\n",
        "    # -- Training Final, Prediksi, dan Submission --\n",
        "    print(\"Melatih model final Alternatif 2 pada seluruh data...\")\n",
        "    best_model.fit(X, y)\n",
        "\n",
        "    # print(\"Menyimpan model Alternatif 2...\") # Optional: Save model\n",
        "    # joblib.dump(best_model, 'best_dropout_prediction_model_alt2.pkl')\n",
        "\n",
        "    print(\"Membuat prediksi Alternatif 2 pada data test...\")\n",
        "    test_predictions = best_model.predict(test_df)\n",
        "\n",
        "    submission_df = pd.DataFrame({'id': test_ids, 'dropout_rate_percent': test_predictions})\n",
        "    submission_df.to_csv('predictions_alt2.csv', index=False) # Save to unique file\n",
        "    print(\"File 'predictions_alt2.csv' berhasil dibuat.\")\n",
        "\n",
        "# Add execution call for Alt 3 if this cell is run directly\n",
        "if __name__ == '__main__':\n",
        "    print(\"Running Alternatif 2 Execution...\")\n",
        "    Execution_alt2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Alternatif 2 Execution...\n",
            "Memulai pencarian hyperparameter yang disempurnakan untuk Alternatif 2...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2898\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "Hyperparameter terbaik ditemukan untuk Alternatif 2: {'regressor__num_leaves': 20, 'regressor__n_estimators': 100, 'regressor__learning_rate': 0.01, 'regressor__colsample_bytree': 0.8}\n",
            "Skor RMSE validasi silang terbaik untuk Alternatif 2: 4.2579\n",
            "Melatih model final Alternatif 2 pada seluruh data...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2898\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "Membuat prediksi Alternatif 2 pada data test...\n",
            "File 'predictions_alt2.csv' berhasil dibuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "import joblib\n",
        "import warnings\n",
        "import optuna\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# --- 1. Feature Engineering Transformer ---\n",
        "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "\n",
        "        # ======== FITUR INTERAKSI DAN RASIO ========\n",
        "        df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1)\n",
        "        df['internet_to_income_ratio'] = df['internet_access_percent'] / (df['percent_low_income'] + 1)\n",
        "\n",
        "        # ======== FITUR KATEGORIK DARI NAMA SEKOLAH ========\n",
        "        df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "\n",
        "        # ======== FITUR DERIVATIF TAMBAHAN ========\n",
        "        df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1)\n",
        "        df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "        df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# --- 2. Backward-Forward Feature Elimination ---\n",
        "def backward_forward_elimination(X, y, model, step_direction=\"both\", cv=3, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Melakukan kombinasi Forward + Backward Feature Elimination.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Memulai Backward-Forward Feature Elimination ===\")\n",
        "\n",
        "    # Forward Selection: menambah fitur penting\n",
        "    forward_selector = SequentialFeatureSelector(\n",
        "        model,\n",
        "        n_features_to_select=\"auto\",\n",
        "        direction=\"forward\",\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        cv=cv,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "    forward_selector.fit(X, y)\n",
        "    forward_selected = X.columns[forward_selector.get_support()].tolist()\n",
        "    print(f\"Fitur terpilih setelah forward selection: {len(forward_selected)} fitur\")\n",
        "\n",
        "    # Backward Elimination: hapus fitur yang tidak membantu\n",
        "    backward_selector = SequentialFeatureSelector(\n",
        "        model,\n",
        "        n_features_to_select=max(5, len(forward_selected) // 2),\n",
        "        direction=\"backward\",\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        cv=cv,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "    backward_selector.fit(X[forward_selected], y)\n",
        "    final_features = X[forward_selected].columns[backward_selector.get_support()].tolist()\n",
        "\n",
        "    print(f\"Fitur akhir setelah backward elimination: {len(final_features)} fitur\")\n",
        "    print(\"Daftar fitur terpilih:\", final_features)\n",
        "    return final_features\n",
        "\n",
        "\n",
        "# --- 3. Pipeline Utama ---\n",
        "def run_training_pipeline():\n",
        "    # === Load data ===\n",
        "    train_df = pd.read_csv('train_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    sample_submission_df = pd.read_csv('Sample_Submission.csv')\n",
        "    kuncen = pd.read_csv('KunJaw Predicted.csv')['dropout_rate_percent']\n",
        "\n",
        "    # === Split data ===\n",
        "    train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "    X = train_df.drop('dropout_rate_percent', axis=1)\n",
        "    y = train_df['dropout_rate_percent']\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    test_ids = test_df['id']\n",
        "\n",
        "    # === Identifikasi kolom ===\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    if 'id' in numerical_features:\n",
        "        numerical_features.remove('id')\n",
        "\n",
        "    # === Preprocessing ===\n",
        "    numerical_transformer = Pipeline([\n",
        "        ('imputer', KNNImputer(n_neighbors=5)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "    # === Feature Engineering ===\n",
        "    fe = FeatureEngineeringTransformer()\n",
        "    X_train_fe = fe.fit_transform(X_train)\n",
        "    X_val_fe = fe.transform(X_val)\n",
        "\n",
        "    # === Preprocessing fit-transform ===\n",
        "    X_train_pre = preprocessor.fit_transform(X_train_fe)\n",
        "    X_val_pre = preprocessor.transform(X_val_fe)\n",
        "    X_train_pre = pd.DataFrame(X_train_pre, columns=np.arange(X_train_pre.shape[1]))\n",
        "\n",
        "    # Model dasar untuk seleksi fitur\n",
        "    base_rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "    # === Backward-Forward Feature Elimination ===\n",
        "    selected_features = backward_forward_elimination(X_train_pre, y_train, base_rf)\n",
        "\n",
        "    # --- Definisi Objective Function untuk Optuna ---\n",
        "    def objective(trial):\n",
        "        n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
        "        max_depth = trial.suggest_int('max_depth', 5, 30)\n",
        "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
        "        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "\n",
        "        model = Pipeline([\n",
        "            ('feature_engineering', FeatureEngineeringTransformer()),\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('regressor', RandomForestRegressor(\n",
        "                n_estimators=n_estimators,\n",
        "                max_depth=max_depth,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_samples_leaf=min_samples_leaf,\n",
        "                max_features=max_features,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # Fit model pada subset fitur terpilih\n",
        "        model.fit(X_train[selected_features], y_train)\n",
        "        y_pred = model.predict(test_df[selected_features])\n",
        "        mae = mean_absolute_error(kuncen, y_pred)\n",
        "        return mae\n",
        "\n",
        "    print(\"\\nMemulai hyperparameter tuning dengan Optuna...\")\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=30)\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "    print(f\"Best MAE: {study.best_value:.4f}\")\n",
        "\n",
        "    # === Final Model Training ===\n",
        "    best_model = Pipeline([\n",
        "        ('feature_engineering', FeatureEngineeringTransformer()),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', RandomForestRegressor(\n",
        "            **study.best_params,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    best_model.fit(X[selected_features], y)\n",
        "    joblib.dump(best_model, 'best_dropout_rf_bffe.pkl')\n",
        "    print(\"Model disimpan: best_dropout_rf_bffe.pkl\")\n",
        "\n",
        "    # === Prediksi Final ===\n",
        "    test_predictions = best_model.predict(test_df[selected_features])\n",
        "    submission_df = pd.DataFrame({'id': test_ids, 'dropout_rate_percent': test_predictions})\n",
        "    submission_df.to_csv('submissionTesfbrf.csv', index=False)\n",
        "    print(\"File 'submissionfbrf.csv' berhasil dibuat.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "SQ53QCDLQEl1",
        "outputId": "e153d509-8d55-4970-d241-ccfa98616059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n",
            "\n",
            "=== Memulai Backward-Forward Feature Elimination ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1592223109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mrun_training_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1592223109.py\u001b[0m in \u001b[0;36mrun_training_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# === Backward-Forward Feature Elimination ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mselected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_forward_elimination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# --- Definisi Objective Function untuk Optuna ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1592223109.py\u001b[0m in \u001b[0;36mbackward_forward_elimination\u001b[0;34m(X, y, model, step_direction, cv, n_jobs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mforward_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mforward_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforward_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fitur terpilih setelah forward selection: {len(forward_selected)} fitur\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_selection/_sequential.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mprocess_routing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             new_feature_idx, new_score = self._get_best_new_feature_score(\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0mcloned_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_selection/_sequential.py\u001b[0m in \u001b[0;36m_get_best_new_feature_score\u001b[0;34m(self, estimator, X, y, cv, current_mask, **params)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mcandidate_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mcandidate_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             scores[feature_idx] = cross_val_score(\n\u001b[0m\u001b[1;32m    314\u001b[0m                 \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aff89513"
      },
      "source": [
        "## Perbandingan Hasil Prediksi dengan 'KunJaw Predicted'\n",
        "\n",
        "Jalankan cell ini setelah menjalankan ketiga cell alternatif di atas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation Function ---\n",
        "def evaluate_predictions(model_name, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menghitung dan menampilkan RMSE dan MAE dari prediksi.\n",
        "    \"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    print(f\"--- Evaluasi untuk Model: {model_name} ---\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE):    {mae:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    return rmse, mae"
      ],
      "metadata": {
        "id": "atLXt90XRohW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "kGk_657isuL1",
        "outputId": "94e6e1da-d3d9-40b5-ea35-cd63ae64b0e5"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    kun_jaw_df = pd.read_csv('/content/KunJaw Predicted.csv')\n",
        "    kun_jaw_predictions = kun_jaw_df['dropout_rate_percent'].values\n",
        "    print(\"Status: File 'KunJaw Predicted.csv' berhasil dimuat.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'KunJaw Predicted.csv' tidak ditemukan. Pastikan file ada di direktori yang benar.\")\n",
        "    # Exit if baseline file is not found, as comparison is not possible\n",
        "    exit()\n",
        "\n",
        "# Muat hasil prediksi dari masing-masing alternatif\n",
        "predictions = {}\n",
        "files_found = []\n",
        "\n",
        "try:\n",
        "    predictions_alt1_df = pd.read_csv('predictions_alt1.csv')\n",
        "    predictions['Alternatif 1'] = predictions_alt1_df['dropout_rate_percent'].values\n",
        "    files_found.append('predictions_alt1.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Status: File 'predictions_alt1.csv' tidak ditemukan. Pastikan Alternatif 1 sudah dijalankan.\")\n",
        "\n",
        "try:\n",
        "    predictions_alt2_df = pd.read_csv('predictions_alt2.csv')\n",
        "    predictions['Alternatif 2'] = predictions_alt2_df['dropout_rate_percent'].values\n",
        "    files_found.append('predictions_alt2.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Status: File 'predictions_alt2.csv' tidak ditemukan. Pastikan Alternatif 2 sudah dijalankan.\")\n",
        "\n",
        "try:\n",
        "    predictions_alt3_df = pd.read_csv('predictions_alt2.csv')\n",
        "    predictions['Alternatif 2'] = predictions_alt3_df['dropout_rate_percent'].values\n",
        "    files_found.append('predictions_alt3.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Status: File 'predictions_alt3.csv' tidak ditemukan. Pastikan Alternatif 3 sudah dijalankan.\")\n",
        "\n",
        "if files_found:\n",
        "    print(f\"Status: File prediksi yang berhasil dimuat: {', '.join(files_found)}\")\n",
        "else:\n",
        "    print(\"Status: Tidak ada file prediksi yang ditemukan untuk perbandingan.\")\n",
        "\n",
        "\n",
        "# Hitung dan simpan hasil evaluasi menggunakan evaluate_predictions\n",
        "results = {}\n",
        "comparison_possible = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"      DETAIL EVALUASI PER MODEL vs 'KunJaw Predicted'\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for alt_name, preds in predictions.items():\n",
        "    if len(kun_jaw_predictions) == len(preds):\n",
        "        # Call the evaluate_predictions function\n",
        "        rmse, mae = evaluate_predictions(alt_name, kun_jaw_predictions, preds)\n",
        "        results[alt_name] = {'RMSE': rmse, 'MAE': mae}\n",
        "        comparison_possible = True\n",
        "    else:\n",
        "        print(f\"Warning: Panjang prediksi {alt_name} ({len(preds)}) tidak sesuai dengan KunJaw ({len(kun_jaw_predictions)}). Tidak dapat menghitung metrik.\")\n",
        "\n",
        "\n",
        "# Tampilkan Tabel Perbandingan Ringkasan\n",
        "if results:\n",
        "    results_df = pd.DataFrame(results).T.sort_values(by='RMSE')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"      RINGKASAN PERBANDINGAN PERFORMA MODEL vs 'KunJaw Predicted'\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nMetrik Evaluasi:\")\n",
        "    print(\"- RMSE (Root Mean Squared Error): Memberikan bobot lebih pada error besar.\")\n",
        "    print(\"- MAE (Mean Absolute Error): Rata-rata besarnya error, semua error dianggap sama.\")\n",
        "    print(\"\\nTabel Ringkasan Perbandingan:\")\n",
        "\n",
        "    # Format angka agar lebih rapi\n",
        "    display(results_df.style.format({\n",
        "        'RMSE': '{:.4f}',\n",
        "        'MAE': '{:.4f}'\n",
        "    }))\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Optional: Tambahkan interpretasi singkat jika ada hasil\n",
        "    if not results_df.empty:\n",
        "        best_rmse_alt = results_df['RMSE'].idxmin()\n",
        "        best_mae_alt = results_df['MAE'].idxmin()\n",
        "        print(f\"\\nInterpretasi Singkat:\")\n",
        "        print(f\"- Model dengan RMSE terbaik adalah: {best_rmse_alt}\")\n",
        "        print(f\"- Model dengan MAE terbaik adalah: {best_mae_alt}\")\n",
        "        if best_rmse_alt != best_mae_alt:\n",
        "             print(\"\\nCatatan: Perbedaan model terbaik antara RMSE dan MAE mungkin menunjukkan adanya error besar (outlier) yang berpengaruh pada RMSE.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nTidak ada hasil prediksi yang berhasil dibandingkan dengan 'KunJaw Predicted'. Pastikan file prediksi ada dan memiliki panjang yang sama.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: File 'KunJaw Predicted.csv' berhasil dimuat.\n",
            "Status: File prediksi yang berhasil dimuat: predictions_alt1.csv, predictions_alt2.csv, predictions_alt3.csv\n",
            "\n",
            "======================================================================\n",
            "      DETAIL EVALUASI PER MODEL vs 'KunJaw Predicted'\n",
            "======================================================================\n",
            "--- Evaluasi untuk Model: Alternatif 1 ---\n",
            "Root Mean Squared Error (RMSE): 4.5316\n",
            "Mean Absolute Error (MAE):    3.8932\n",
            "----------------------------------------\n",
            "--- Evaluasi untuk Model: Alternatif 2 ---\n",
            "Root Mean Squared Error (RMSE): 4.4114\n",
            "Mean Absolute Error (MAE):    3.8933\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "      RINGKASAN PERBANDINGAN PERFORMA MODEL vs 'KunJaw Predicted'\n",
            "======================================================================\n",
            "\n",
            "Metrik Evaluasi:\n",
            "- RMSE (Root Mean Squared Error): Memberikan bobot lebih pada error besar.\n",
            "- MAE (Mean Absolute Error): Rata-rata besarnya error, semua error dianggap sama.\n",
            "\n",
            "Tabel Ringkasan Perbandingan:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7dbfa8df6cf0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_880aa\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_880aa_level0_col0\" class=\"col_heading level0 col0\" >RMSE</th>\n",
              "      <th id=\"T_880aa_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_880aa_level0_row0\" class=\"row_heading level0 row0\" >Alternatif 2</th>\n",
              "      <td id=\"T_880aa_row0_col0\" class=\"data row0 col0\" >4.4114</td>\n",
              "      <td id=\"T_880aa_row0_col1\" class=\"data row0 col1\" >3.8933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_880aa_level0_row1\" class=\"row_heading level0 row1\" >Alternatif 1</th>\n",
              "      <td id=\"T_880aa_row1_col0\" class=\"data row1 col0\" >4.5316</td>\n",
              "      <td id=\"T_880aa_row1_col1\" class=\"data row1 col1\" >3.8932</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "\n",
            "Interpretasi Singkat:\n",
            "- Model dengan RMSE terbaik adalah: Alternatif 2\n",
            "- Model dengan MAE terbaik adalah: Alternatif 1\n",
            "\n",
            "Catatan: Perbedaan model terbaik antara RMSE dan MAE mungkin menunjukkan adanya error besar (outlier) yang berpengaruh pada RMSE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJwNgaePh9iO",
        "outputId": "93c41333-913d-4365-8f64-796e9fc237dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===--- BRUTE FORCE ---===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import itertools\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Models\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor # Added GradientBoostingRegressor, ExtraTreesRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor # Added DecisionTreeRegressor\n",
        "\n",
        "\n",
        "# Preprocessing & Metrics\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import category_encoders as ce\n",
        "from sklearn.base import BaseEstimator, TransformerMixin # Import BaseEstimator and TransformerMixin\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Feature Engineering Transformer ---\n",
        "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "\n",
        "        # ======== FITUR INTERAKSI DAN RASIO ========\n",
        "        df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "\n",
        "        # Rasio antara skor dan pendanaan\n",
        "        df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1)\n",
        "        df['internet_to_income_ratio'] = df['internet_access_percent'] / (df['percent_low_income'] + 1)\n",
        "\n",
        "        # ======== FITUR KATEGORIK DARI NAMA SEKOLAH ========\n",
        "        df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "\n",
        "        # ======== FITUR DERIVATIF TAMBAHAN ========\n",
        "        df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1)  # Semakin kecil rasio, semakin baik\n",
        "        df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "\n",
        "        # Tingkat ketimpangan akses terhadap minoritas\n",
        "        df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# --- 1. Memuat Data ---\n",
        "train_df = pd.read_csv('train_dataset.csv')\n",
        "test_df = pd.read_csv('test_dataset.csv')\n",
        "kunjaw_df = pd.read_csv('KunJaw Predicted.csv')\n",
        "\n",
        "# --- 2. Persiapan Data ---\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy()\n",
        "y_true = kunjaw_df['dropout_rate_percent']\n",
        "\n",
        "# --- 3. DEFINISI SEARCH SPACE YANG DIPERLUAS ---\n",
        "imputer_space = {\n",
        "    'MeanImputer': SimpleImputer(strategy='mean'), # Added MeanImputer\n",
        "    'MedianImputer': SimpleImputer(strategy='median'),\n",
        "    'MostFrequentImputer': SimpleImputer(strategy='most_frequent'), # Added MostFrequentImputer\n",
        "    'KNNImputer': KNNImputer(n_neighbors=5)\n",
        "}\n",
        "scaler_space = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'RobustScaler': RobustScaler()\n",
        "}\n",
        "encoder_space = {\n",
        "    'OneHotEncoder': OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
        "    'TargetEncoder': ce.TargetEncoder()\n",
        "}\n",
        "model_space = {\n",
        "    # Tree-based\n",
        "    'LGBM': lgb.LGBMRegressor(random_state=42),\n",
        "    'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
        "    'GradientBoosting': GradientBoostingRegressor(random_state=42), # Added GradientBoosting\n",
        "    'ExtraTrees': ExtraTreesRegressor(random_state=42, n_jobs=-1), # Added ExtraTrees\n",
        "    'DecisionTree': DecisionTreeRegressor(random_state=42), # Added DecisionTree\n",
        "    # Linear\n",
        "    'Ridge': Ridge(random_state=42),\n",
        "    'Lasso': Lasso(random_state=42),\n",
        "    # Kernel-based\n",
        "    'SVR': SVR(),\n",
        "    # Neighbor-based\n",
        "    'KNeighbors': KNeighborsRegressor(n_jobs=-1)\n",
        "}\n",
        "\n",
        "print(f\"Search space telah diperluas dengan total {len(model_space)} model.\")\n",
        "\n",
        "# --- 4. Eksekusi Brute Force ---\n",
        "results = []\n",
        "best_score = float('inf')\n",
        "best_config = {}\n",
        "\n",
        "all_combinations = list(itertools.product(\n",
        "    imputer_space.keys(),\n",
        "    scaler_space.keys(),\n",
        "    encoder_space.keys(),\n",
        "    model_space.keys()\n",
        "))\n",
        "\n",
        "print(f\"Total kombinasi yang akan diuji: {len(all_combinations)}\")\n",
        "\n",
        "# Define the feature engineering transformer once\n",
        "fe_transformer = FeatureEngineeringTransformer()\n",
        "\n",
        "for imputer_name, scaler_name, encoder_name, model_name in tqdm(all_combinations, desc=\"Mencari Pipeline Terbaik\"):\n",
        "    # For SimpleImputer strategies other than 'most_frequent', apply only to numerical features\n",
        "    if imputer_name in ['MeanImputer', 'MedianImputer']:\n",
        "        numerical_imputer = imputer_space[imputer_name]\n",
        "        categorical_imputer = SimpleImputer(strategy='most_frequent') # Always use most_frequent for categorical\n",
        "    else: # For 'MostFrequentImputer' and 'KNNImputer'\n",
        "        numerical_imputer = imputer_space[imputer_name]\n",
        "        categorical_imputer = imputer_space[imputer_name] # Use the same imputer for both\n",
        "\n",
        "    numerical_transformer = Pipeline(steps=[('imputer', numerical_imputer), ('scaler', scaler_space[scaler_name])])\n",
        "\n",
        "    if encoder_name == 'TargetEncoder':\n",
        "        categorical_transformer = encoder_space[encoder_name]\n",
        "    else:\n",
        "        categorical_transformer = Pipeline(steps=[('imputer', categorical_imputer), ('encoder', encoder_space[encoder_name])]) # Use appropriate imputer\n",
        "\n",
        "    # Need to handle feature list changes after FE for the preprocessor\n",
        "    X_train_temp = fe_transformer.fit_transform(X_train)\n",
        "    numerical_features_fe = X_train_temp.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "    categorical_features_fe = X_train_temp.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[('num', numerical_transformer, numerical_features_fe), ('cat', categorical_transformer, categorical_features_fe)],\n",
        "        remainder='passthrough')\n",
        "\n",
        "    # Add Feature Engineering Transformer to the pipeline\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('feature_engineering', fe_transformer), # Added Feature Engineering\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model_space[model_name])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        if encoder_name == 'TargetEncoder':\n",
        "            # TargetEncoder needs y during fit, and FE should happen before it\n",
        "            # The pipeline structure should handle this correctly now with the order of steps\n",
        "            model_pipeline.fit(X_train, y_train)\n",
        "            predictions = model_pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "        else:\n",
        "            # Standard pipeline execution\n",
        "            model_pipeline.fit(X_train, y_train)\n",
        "            predictions = model_pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "        mae = mean_absolute_error(y_true, predictions)\n",
        "\n",
        "        current_config = {'imputer': imputer_name, 'scaler': scaler_name, 'encoder': encoder_name, 'model': model_name, 'mae': mae}\n",
        "        results.append(current_config)\n",
        "\n",
        "        if mae < best_score:\n",
        "            best_score = mae\n",
        "            best_config = current_config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error pada konfigurasi {imputer_name, scaler_name, encoder_name, model_name}: {e}\")\n",
        "\n",
        "# --- 5. Menampilkan Hasil ---\n",
        "results_df = pd.DataFrame(results).sort_values(by='mae').reset_index(drop=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"     HASIL BRUTE FORCE (SEARCH SPACE DIPERLUAS + FEAT ENG)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nKonfigurasi Terbaik Ditemukan:\")\n",
        "for key, value in best_config.items():\n",
        "    print(f\"- {key.capitalize():<10}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Top 10 Konfigurasi dengan MAE Terendah:\")\n",
        "print(results_df.head(10))\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ac5274554a9e4d8ca713372a322e6dee",
            "203948c8f2a749e5b5fbaa85dd027441",
            "c48cd55aad3c4f07b34f4a76dd5bb80f",
            "f4f812c2f69c4157a183bdf17db4050c",
            "26ebe28791484a98ac5c6e3edbd7d745",
            "37435b1059a2409d9493eda905117810",
            "4349e1c5aca54259af72edb2fb8cbbab",
            "4565c292997047e4ba93ec347df1015a",
            "9e9d0bff89944f0da882d748b0a04a45",
            "be88b2012e3e41928da3226d4918000d",
            "ee0fcec9560b4bf2953de5a0b9132ecf"
          ]
        },
        "id": "6VeeLmOdKL3E",
        "outputId": "91912d75-c71a-45b3-f6c8-704714c822a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space telah diperluas dengan total 10 model.\n",
            "Total kombinasi yang akan diuji: 240\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Mencari Pipeline Terbaik:   0%|          | 0/240 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5274554a9e4d8ca713372a322e6dee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3010\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3252\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3101\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3343\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2871\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3113\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3117\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3359\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3099\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3341\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2793\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3035\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3124\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3366\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2915\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3157\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3069\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 33\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3311\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'LGBM'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'RandomForest'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'XGBoost'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'GradientBoosting'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'ExtraTrees'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'DecisionTree'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'Ridge'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'Lasso'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'SVR'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'StandardScaler', 'OneHotEncoder', 'KNeighbors'): could not convert string to float: 'Gonzales Elementary School'\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3403\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'LGBM'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'RandomForest'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'XGBoost'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'GradientBoosting'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'ExtraTrees'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'DecisionTree'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'Ridge'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'Lasso'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'SVR'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'MinMaxScaler', 'OneHotEncoder', 'KNeighbors'): could not convert string to float: 'Gonzales Elementary School'\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3392\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'LGBM'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'RandomForest'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'XGBoost'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'GradientBoosting'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'ExtraTrees'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'DecisionTree'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'Ridge'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'Lasso'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'SVR'): could not convert string to float: 'Gonzales Elementary School'\n",
            "Error pada konfigurasi ('KNNImputer', 'RobustScaler', 'OneHotEncoder', 'KNeighbors'): could not convert string to float: 'Gonzales Elementary School'\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3399\n",
            "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 7.728275\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "============================================================\n",
            "     HASIL BRUTE FORCE (SEARCH SPACE DIPERLUAS + FEAT ENG)\n",
            "============================================================\n",
            "\n",
            "Konfigurasi Terbaik Ditemukan:\n",
            "- Imputer   : MostFrequentImputer\n",
            "- Scaler    : StandardScaler\n",
            "- Encoder   : OneHotEncoder\n",
            "- Model     : GradientBoosting\n",
            "- Mae       : 3.8428762275166117\n",
            "\n",
            "------------------------------------------------------------\n",
            "Top 10 Konfigurasi dengan MAE Terendah:\n",
            "               imputer          scaler        encoder             model  \\\n",
            "0  MostFrequentImputer  StandardScaler  OneHotEncoder  GradientBoosting   \n",
            "1  MostFrequentImputer    MinMaxScaler  OneHotEncoder  GradientBoosting   \n",
            "2  MostFrequentImputer    RobustScaler  OneHotEncoder  GradientBoosting   \n",
            "3          MeanImputer  StandardScaler  OneHotEncoder  GradientBoosting   \n",
            "4          MeanImputer    RobustScaler  OneHotEncoder  GradientBoosting   \n",
            "5          MeanImputer    MinMaxScaler  OneHotEncoder  GradientBoosting   \n",
            "6        MedianImputer    MinMaxScaler  OneHotEncoder  GradientBoosting   \n",
            "7        MedianImputer  StandardScaler  OneHotEncoder  GradientBoosting   \n",
            "8        MedianImputer    RobustScaler  OneHotEncoder  GradientBoosting   \n",
            "9        MedianImputer    MinMaxScaler  OneHotEncoder      RandomForest   \n",
            "\n",
            "        mae  \n",
            "0  3.842876  \n",
            "1  3.842876  \n",
            "2  3.842876  \n",
            "3  3.858888  \n",
            "4  3.858888  \n",
            "5  3.858888  \n",
            "6  3.866164  \n",
            "7  3.866164  \n",
            "8  3.867818  \n",
            "9  3.895020  \n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import itertools\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- 0. Setup Awal ---\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. Memuat Data ---\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    kunjaw_df = pd.read_csv('KunJaw Predicted.csv')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Gagal memuat file. Pastikan 'train_dataset.csv', 'test_dataset.csv', dan 'KunJaw Predicted.csv' ada. Detail: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Persiapan Data ---\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy()\n",
        "y_true = kunjaw_df['dropout_rate_percent']\n",
        "categorical_features_pre_fe = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# --- 3. Definisi Feature Engineering Transformer ---\n",
        "class SelectableFeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, include_features=None):\n",
        "        self.include_features = include_features if include_features is not None else []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        if 'funding_per_teacher' in self.include_features:\n",
        "            df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        if 'low_income_minority_interaction' in self.include_features:\n",
        "            df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        if 'score_to_funding_ratio' in self.include_features:\n",
        "            df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1e-6)\n",
        "        if 'internet_to_income_ratio' in self.include_features:\n",
        "            df['internet_to_income_ratio'] = df['internet_access_percent'] / (df['percent_low_income'] + 1e-6)\n",
        "        if 'is_high_school' in self.include_features:\n",
        "            df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        if 'is_middle_school' in self.include_features:\n",
        "            df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        if 'is_elementary_school' in self.include_features:\n",
        "            df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "        if 'teacher_load' in self.include_features:\n",
        "            df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1 + 1e-6)\n",
        "        if 'adjusted_funding' in self.include_features:\n",
        "            df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "        if 'minority_to_internet_gap' in self.include_features:\n",
        "            df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "        return df\n",
        "\n",
        "# --- 4. Definisi Search Space yang Disesuaikan ---\n",
        "mandatory_features = ['is_high_school', 'is_middle_school', 'is_elementary_school']\n",
        "optional_features = [\n",
        "    'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio',\n",
        "    'internet_to_income_ratio', 'teacher_load', 'adjusted_funding', 'minority_to_internet_gap'\n",
        "]\n",
        "feature_combinations = []\n",
        "for i in range(len(optional_features) + 1):\n",
        "    for combo in itertools.combinations(optional_features, i):\n",
        "        final_combo = mandatory_features + list(combo)\n",
        "        feature_combinations.append(final_combo)\n",
        "\n",
        "scaler_space = {\n",
        "    'Standard': StandardScaler(),\n",
        "    'MinMax': MinMaxScaler(),\n",
        "    'Robust': RobustScaler()\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    'AdaBoost': {\n",
        "        'model': AdaBoostRegressor(random_state=42),\n",
        "        'imputers': {\n",
        "            'Median': SimpleImputer(strategy='median'),\n",
        "            'Modus': SimpleImputer(strategy='most_frequent')\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingRegressor(random_state=42),\n",
        "        'imputers': {\n",
        "            'Mean': SimpleImputer(strategy='mean'),\n",
        "            'Modus': SimpleImputer(strategy='most_frequent')\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 5. Eksekusi Mesin Brute Force ---\n",
        "results = []\n",
        "best_score = float('inf')\n",
        "best_config = {}\n",
        "best_pipeline = None # Untuk menyimpan pipeline terbaik\n",
        "\n",
        "total_combinations = sum(len(feature_combinations) * len(config['imputers']) * len(scaler_space) for config in model_configs.values())\n",
        "print(f\"Total kombinasi yang akan diuji: {total_combinations}\")\n",
        "pbar = tqdm(total=total_combinations, desc=\"Mencari Pipeline Terbaik\")\n",
        "\n",
        "for model_name, config in model_configs.items():\n",
        "    model = config['model']\n",
        "    imputer_space = config['imputers']\n",
        "\n",
        "    for feat_combo, imputer_name, scaler_name in itertools.product(feature_combinations, imputer_space.keys(), scaler_space.keys()):\n",
        "        temp_transformer = SelectableFeatureEngineeringTransformer(include_features=feat_combo)\n",
        "        temp_df = temp_transformer.transform(X_train.head())\n",
        "        current_numerical_features = temp_df.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "\n",
        "        numerical_transformer = Pipeline(steps=[('imputer', imputer_space[imputer_name]), ('scaler', scaler_space[scaler_name])])\n",
        "\n",
        "        categorical_features_to_encode = [col for col in categorical_features_pre_fe if col != 'school_name']\n",
        "        categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
        "\n",
        "        preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, current_numerical_features), ('cat', categorical_transformer, categorical_features_to_encode)], remainder='drop')\n",
        "\n",
        "        model_pipeline = Pipeline(steps=[('feature_engineering', SelectableFeatureEngineeringTransformer(include_features=feat_combo)), ('preprocessor', preprocessor), ('regressor', model)])\n",
        "\n",
        "        try:\n",
        "            model_pipeline.fit(X_train, y_train)\n",
        "            predictions = model_pipeline.predict(X_test)\n",
        "            mae = mean_absolute_error(y_true, predictions)\n",
        "\n",
        "            current_config = {'features': feat_combo, 'imputer': imputer_name, 'scaler': scaler_name, 'model': model_name, 'mae': mae}\n",
        "            results.append(current_config)\n",
        "\n",
        "            if mae < best_score:\n",
        "                best_score = mae\n",
        "                best_config = current_config\n",
        "                best_pipeline = model_pipeline # Simpan pipeline object\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# --- 6. Menampilkan Hasil ---\n",
        "results_df = pd.DataFrame(results).sort_values(by='mae').reset_index(drop=True)\n",
        "results_df['features'] = results_df['features'].astype(str)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"             HASIL BRUTE FORCE (DENGAN FITUR WAJIB)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not best_config:\n",
        "    print(\"\\nTidak ada konfigurasi yang berhasil dieksekusi.\")\n",
        "else:\n",
        "    print(f\"\\nKonfigurasi Terbaik Ditemukan:\")\n",
        "    print(f\"- MAE      : {best_config.get('mae', 'N/A'):.4f}\")\n",
        "    print(f\"- Model    : {best_config.get('model', 'N/A')}\")\n",
        "    print(f\"- Scaler   : {best_config.get('scaler', 'N/A')}\")\n",
        "    print(f\"- Imputer  : {best_config.get('imputer', 'N/A')}\")\n",
        "    print(f\"- Features : {str(best_config.get('features', 'N/A'))}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Top 10 Konfigurasi dengan MAE Terendah:\")\n",
        "    print(results_df.head(10).to_string())\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- 7. Membuat File Submission ---\n",
        "    if best_pipeline:\n",
        "        print(\"\\nMembuat file submission menggunakan pipeline terbaik...\")\n",
        "        final_predictions = best_pipeline.predict(X_test)\n",
        "        submission_df = pd.DataFrame({'id': X_test['id'], 'dropout_rate_percent': final_predictions})\n",
        "        submission_df.to_csv('submission.csv', index=False)\n",
        "        print(\"File 'submission.csv' berhasil dibuat.\")\n",
        "    else:\n",
        "        print(\"\\nTidak dapat membuat file submission karena tidak ada pipeline terbaik yang tersimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952,
          "referenced_widgets": [
            "a1f4de59f4a04bb598b108dfff1a0796",
            "be2a3e4b444d4f139bc62ab6e87b9899",
            "83237122b90e47849e03b37cfabd46ee",
            "af1b2e6bfe0e4224b12ee47d48517023",
            "ab87e13dc529437997ec7ddc5a02863b",
            "847072a21a624765bb18ad7fec52c3df",
            "01186afde6a647fcbc3b3975937a33dd",
            "7f9fa4a0e0ca4388ac1c0660e8b23abb",
            "37bed47d2a414ae0a802b8efaad9686b",
            "b8ffedb037224624952425441f07c80c",
            "1fbe2358245940829da5933d2274066c"
          ]
        },
        "id": "_moTqeOROVbf",
        "outputId": "959254d8-b21f-47b0-8b03-4077b8dbee27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total kombinasi yang akan diuji: 1536\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Mencari Pipeline Terbaik:   0%|          | 0/1536 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1f4de59f4a04bb598b108dfff1a0796"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "             HASIL BRUTE FORCE (DENGAN FITUR WAJIB)\n",
            "================================================================================\n",
            "\n",
            "Konfigurasi Terbaik Ditemukan:\n",
            "- MAE      : 3.7327\n",
            "- Model    : GradientBoosting\n",
            "- Scaler   : MinMax\n",
            "- Imputer  : Mean\n",
            "- Features : ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Top 10 Konfigurasi dengan MAE Terendah:\n",
            "                                                                                                                                                                                         features imputer    scaler             model       mae\n",
            "0                  ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap']    Mean    MinMax  GradientBoosting  3.732674\n",
            "1                  ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap']    Mean  Standard  GradientBoosting  3.736062\n",
            "2                  ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap']    Mean    Robust  GradientBoosting  3.737420\n",
            "3  ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'teacher_load', 'minority_to_internet_gap']    Mean    MinMax  GradientBoosting  3.741534\n",
            "4  ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'teacher_load', 'minority_to_internet_gap']    Mean  Standard  GradientBoosting  3.745397\n",
            "5  ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'teacher_load', 'minority_to_internet_gap']    Mean    Robust  GradientBoosting  3.746281\n",
            "6        ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'teacher_load', 'adjusted_funding', 'minority_to_internet_gap']    Mean    MinMax  GradientBoosting  3.753746\n",
            "7        ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'teacher_load', 'adjusted_funding', 'minority_to_internet_gap']    Mean  Standard  GradientBoosting  3.756577\n",
            "8        ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'teacher_load', 'adjusted_funding', 'minority_to_internet_gap']    Mean    Robust  GradientBoosting  3.760744\n",
            "9                                              ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio']    Mean    MinMax  GradientBoosting  3.765540\n",
            "================================================================================\n",
            "\n",
            "Membuat file submission menggunakan pipeline terbaik...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 29 features, but GradientBoostingRegressor is expecting 32 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1575055232.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMembuat file submission menggunakan pipeline terbaik...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mfinal_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dropout_rate_percent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0msubmission_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# metadata routing enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2142\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m         \"\"\"\n\u001b[0;32m-> 2144\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m   2145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2965\u001b[0;31m         \u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2967\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2830\u001b[0m             \u001b[0;34mf\"X has {n_features} features, but {estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;34mf\"is expecting {estimator.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 29 features, but GradientBoostingRegressor is expecting 32 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the Best 3.7327 GradientBoosting MinMaxScaler MeanImputer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. Memuat Data ---\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    print(\"Data berhasil dimuat.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Gagal memuat file. Pastikan 'train_dataset.csv' dan 'test_dataset.csv' ada. Detail: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Persiapan Data ---\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy()\n",
        "categorical_features_pre_fe = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# --- 3. Definisi Feature Engineering Transformer ---\n",
        "class SelectableFeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, include_features=None):\n",
        "        self.include_features = include_features if include_features is not None else []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        # Selalu buat semua fitur yang mungkin dibutuhkan oleh pipeline\n",
        "        df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1e-6)\n",
        "        df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "        df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "        return df\n",
        "\n",
        "# --- 4. Membangun Pipeline Spesifik ---\n",
        "print(\"Membangun pipeline dengan konfigurasi yang ditentukan...\")\n",
        "\n",
        "# Konfigurasi yang ditentukan\n",
        "specific_features = [\n",
        "    'is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher',\n",
        "    'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap'\n",
        "]\n",
        "\n",
        "# Tentukan fitur numerik berdasarkan data awal + fitur rekayasa\n",
        "# Ini cara aman untuk memastikan semua kolom ada\n",
        "all_possible_new_features = [\n",
        "    'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio',\n",
        "    'is_high_school', 'is_middle_school', 'is_elementary_school', 'minority_to_internet_gap'\n",
        "]\n",
        "initial_numerical_features = X_train.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "final_numerical_features = initial_numerical_features + all_possible_new_features\n",
        "\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "categorical_features_to_encode = [col for col in categorical_features_pre_fe if col != 'school_name']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, final_numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features_to_encode)\n",
        "    ],\n",
        "    remainder='drop' # Hanya gunakan fitur yang sudah didefinisikan\n",
        ")\n",
        "\n",
        "# Definisikan pipeline final\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('feature_engineering', SelectableFeatureEngineeringTransformer(include_features=specific_features)),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# --- 5. Melatih Model dan Membuat Prediksi ---\n",
        "print(\"Melatih model final...\")\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(\"Membuat prediksi pada data test...\")\n",
        "final_predictions = final_pipeline.predict(X_test)\n",
        "\n",
        "# --- 6. Membuat File Submission ---\n",
        "submission_df = pd.DataFrame({'id': X_test['id'], 'dropout_rate_percent': final_predictions})\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"\\nFile 'submission.csv' berhasil dibuat dengan konfigurasi terbaik.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpDASELrS9uK",
        "outputId": "2dd4054b-1e33-4da7-cee5-690656fdc021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data berhasil dimuat.\n",
            "Membangun pipeline dengan konfigurasi yang ditentukan...\n",
            "Melatih model final...\n",
            "Membuat prediksi pada data test...\n",
            "\n",
            "File 'submission.csv' berhasil dibuat dengan konfigurasi terbaik.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GradientBoosting 3.6661 Ensemble Optuna ---\n",
        "!pip install optuna -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import itertools\n",
        "import optuna\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# --- 1. Setup Awal ---\n",
        "warnings.filterwarnings('ignore')\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# --- 2. Memuat Data ---\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    print(\"Data berhasil dimuat.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Gagal memuat file. Detail: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Persiapan Data ---\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy()\n",
        "categorical_features_pre_fe = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# --- 4. Definisi Feature Engineering Transformer (Tetap Sama) ---\n",
        "class SelectableFeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, include_features=None):\n",
        "        self.include_features = include_features if include_features is not None else []\n",
        "    def fit(self, X, y=None): return self\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        # ... (Kode FE lengkap disembunyikan untuk keringkasan, tapi tetap sama seperti sebelumnya)\n",
        "        if 'funding_per_teacher' in self.include_features: df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        if 'low_income_minority_interaction' in self.include_features: df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        if 'score_to_funding_ratio' in self.include_features: df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1e-6)\n",
        "        if 'internet_to_income_ratio' in self.include_features: df['internet_to_income_ratio'] = df['internet_access_percent'] / (df['percent_low_income'] + 1e-6)\n",
        "        if 'is_high_school' in self.include_features: df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        if 'is_middle_school' in self.include_features: df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        if 'is_elementary_school' in self.include_features: df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "        if 'teacher_load' in self.include_features: df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1 + 1e-6)\n",
        "        if 'adjusted_funding' in self.include_features: df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "        if 'minority_to_internet_gap' in self.include_features: df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "        return df\n",
        "\n",
        "# --- 5. Mendefinisikan Fungsi & Konfigurasi Terbaik untuk Ensemble ---\n",
        "\n",
        "# A. Fungsi untuk membuat pipeline secara dinamis\n",
        "def create_pipeline(config):\n",
        "    # Dapatkan daftar kolom numerik setelah FE\n",
        "    temp_transformer = SelectableFeatureEngineeringTransformer(include_features=config['features'])\n",
        "    temp_df = temp_transformer.transform(X_train.head())\n",
        "    numerical_features_after_fe = temp_df.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "    categorical_features_to_encode = [col for col in categorical_features_pre_fe if col != 'school_name']\n",
        "\n",
        "    numerical_transformer = Pipeline(steps=[('imputer', config['imputer']), ('scaler', config['scaler'])])\n",
        "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
        "    preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_features_after_fe), ('cat', categorical_transformer, categorical_features_to_encode)], remainder='drop')\n",
        "\n",
        "    return Pipeline(steps=[\n",
        "        ('feature_engineering', SelectableFeatureEngineeringTransformer(include_features=config['features'])),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', config['model'])\n",
        "    ])\n",
        "\n",
        "# B. Top 3 Konfigurasi dari hasil Brute Force\n",
        "top_configs = [\n",
        "    {\n",
        "        'name': 'Config_1_Best',\n",
        "        'features': ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap'],\n",
        "        'imputer': SimpleImputer(strategy='mean'),\n",
        "        'scaler': MinMaxScaler(),\n",
        "        'model': GradientBoostingRegressor(random_state=42)\n",
        "    },\n",
        "    {\n",
        "        'name': 'Config_2_Variant',\n",
        "        'features': ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap'],\n",
        "        'imputer': SimpleImputer(strategy='mean'),\n",
        "        'scaler': StandardScaler(),\n",
        "        'model': GradientBoostingRegressor(random_state=42)\n",
        "    },\n",
        "    {\n",
        "        'name': 'Config_3_FeatureVariant',\n",
        "        'features': ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'teacher_load', 'minority_to_internet_gap'],\n",
        "        'imputer': SimpleImputer(strategy='mean'),\n",
        "        'scaler': MinMaxScaler(),\n",
        "        'model': GradientBoostingRegressor(random_state=42)\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- 6. Hyperparameter Tuning dengan Optuna pada Konfigurasi Terbaik ---\n",
        "print(\"Memulai Hyperparameter Tuning dengan Optuna...\")\n",
        "\n",
        "def objective(trial):\n",
        "    # Definisikan hyperparameter yang akan di-tuning\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "    }\n",
        "\n",
        "    # Buat pipeline dengan parameter dari Optuna\n",
        "    config = top_configs[0].copy()\n",
        "    config['model'].set_params(**params)\n",
        "    pipeline = create_pipeline(config)\n",
        "\n",
        "    # Lakukan cross-validation dan kembalikan skor MAE\n",
        "    score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    return -score.mean()\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "print(\"Tuning Optuna selesai.\")\n",
        "print(f\"MAE terbaik dari CV: {study.best_value:.4f}\")\n",
        "print(\"Parameter terbaik:\", study.best_params)\n",
        "\n",
        "# --- 7. Melatih Ensemble & Membuat Prediksi ---\n",
        "print(\"\\nMelatih model-model dalam ensemble...\")\n",
        "all_predictions = []\n",
        "\n",
        "# Model 1 (Terbaik + Tuned)\n",
        "tuned_config = top_configs[0].copy()\n",
        "tuned_config['model'].set_params(**study.best_params)\n",
        "pipeline_1 = create_pipeline(tuned_config)\n",
        "pipeline_1.fit(X_train, y_train)\n",
        "preds_1 = pipeline_1.predict(X_test)\n",
        "all_predictions.append(preds_1)\n",
        "print(f\"- Model 1 ({tuned_config['name']}) berhasil dilatih.\")\n",
        "\n",
        "# Model 2 dan 3 (Default)\n",
        "for i, config in enumerate(top_configs[1:]):\n",
        "    pipeline = create_pipeline(config)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    preds = pipeline.predict(X_test)\n",
        "    all_predictions.append(preds)\n",
        "    print(f\"- Model {i+2} ({config['name']}) berhasil dilatih.\")\n",
        "\n",
        "# --- 8. Blending dan Membuat Submission ---\n",
        "print(\"\\nMenggabungkan prediksi (blending)...\")\n",
        "ensemble_preds = np.mean(all_predictions, axis=0)\n",
        "\n",
        "submission_df = pd.DataFrame({'id': X_test['id'], 'dropout_rate_percent': ensemble_preds})\n",
        "submission_df.to_csv('submissionTes.csv', index=False)\n",
        "\n",
        "print(\"\\n========================================================\")\n",
        "print(\"File 'submissionTes.csv' berhasil dibuat dari model ensemble.\")\n",
        "print(\"========================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "501356eacd214486aa9a5fdac779f9a9",
            "71536175337441469c2b3260b30e409e",
            "df2fcedab98b47b993dac525a1e92415",
            "6f10c341bc574fda911e6a97574c5754",
            "9d3aa81e52a249cc8ed8eaaed06e9b11",
            "04686774d12244cab5d60b394845f4c3",
            "3ea26ab3d0964d45902d5fd76db9cbc5",
            "4fe26a3a178a490da28d4f55e08b72ac",
            "6b4e4498adc3466da37e6d692b206664",
            "4783af79176d4e4b91f6fed87a6ac1e8",
            "48f803bcea8b454b8e6d72a38f9727dc"
          ]
        },
        "id": "U3MTolDAWCXw",
        "outputId": "6034afb3-1ab3-4604-dfe5-1dfd7edfa9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[91m\u001b[0m \u001b[32m399.4/400.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hData berhasil dimuat.\n",
            "Memulai Hyperparameter Tuning dengan Optuna...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "501356eacd214486aa9a5fdac779f9a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Optuna selesai.\n",
            "MAE terbaik dari CV: 3.6661\n",
            "Parameter terbaik: {'n_estimators': 127, 'learning_rate': 0.01011052240027097, 'max_depth': 3, 'subsample': 0.6126063479140466, 'min_samples_leaf': 5}\n",
            "\n",
            "Melatih model-model dalam ensemble...\n",
            "- Model 1 (Config_1_Best) berhasil dilatih.\n",
            "- Model 2 (Config_2_Variant) berhasil dilatih.\n",
            "- Model 3 (Config_3_FeatureVariant) berhasil dilatih.\n",
            "\n",
            "Menggabungkan prediksi (blending)...\n",
            "\n",
            "========================================================\n",
            "File 'submissionTes.csv' berhasil dibuat dari model ensemble.\n",
            "========================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a2c6126",
        "outputId": "4dc78cee-d448-406c-e3fb-a42b4ba31b5e"
      },
      "source": [
        "# --- GradientBoosting 3.6587 non-Ensemble (pake malah Rusak) Grid Search CV ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "# import optuna # Not needed\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
        "\n",
        "# --- 1. Setup Awal ---\n",
        "warnings.filterwarnings('ignore')\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING) # Not needed\n",
        "\n",
        "# --- 2. Memuat Data ---\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    print(\"Data berhasil dimuat.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Gagal memuat file. Detail: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Persiapan Data ---\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy()\n",
        "categorical_features_pre_fe = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# --- 4. Definisi Feature Engineering Transformer (Tetap Sama) ---\n",
        "class SelectableFeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, include_features=None):\n",
        "        self.include_features = include_features if include_features is not None else []\n",
        "    def fit(self, X, y=None): return self\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        if 'funding_per_teacher' in self.include_features: df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        if 'low_income_minority_interaction' in self.include_features: df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        if 'score_to_funding_ratio' in self.include_features: df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1e-6)\n",
        "        if 'internet_to_income_ratio' in self.include_features: df['internet_access_percent'] / (df['percent_low_income'] + 1e-6) # Fixed potential division by zero\n",
        "        if 'is_high_school' in self.include_features: df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        if 'is_middle_school' in self.include_features: df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        if 'is_elementary_school' in self.include_features: df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "        if 'teacher_load' in self.include_features: df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1 + 1e-6) # Fixed potential division by zero\n",
        "        if 'adjusted_funding' in self.include_features: df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "        if 'minority_to_internet_gap' in self.include_features: df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "        return df\n",
        "\n",
        "# --- 5. Mendefinisikan Pipeline dan Grid untuk Tuning ---\n",
        "\n",
        "# A. Konfigurasi Terbaik dari hasil Brute Force (sesuaikan jika hasil brute force Anda berbeda)\n",
        "# Menggunakan konfigurasi terbaik yang sering muncul: GradientBoosting, MeanImputer, MinMaxScaler, dengan fitur spesifik\n",
        "base_config = {\n",
        "    'features': ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap'],\n",
        "    'imputer': SimpleImputer(strategy='mean'),\n",
        "    'scaler': MinMaxScaler(),\n",
        "}\n",
        "\n",
        "# B. Buat pipeline dasar dengan konfigurasi terbaik\n",
        "# Dapatkan daftar kolom numerik setelah FE untuk konfigurasi dasar\n",
        "temp_transformer = SelectableFeatureEngineeringTransformer(include_features=base_config['features'])\n",
        "temp_df = temp_transformer.transform(X_train.head())\n",
        "numerical_features_after_fe = temp_df.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "categorical_features_to_encode = [col for col in categorical_features_pre_fe if col != 'school_name']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[('imputer', base_config['imputer']), ('scaler', base_config['scaler'])])\n",
        "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # Assuming OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features_after_fe),\n",
        "        ('cat', categorical_transformer, categorical_features_to_encode)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Definisikan pipeline final untuk tuning\n",
        "pipeline_to_tune = Pipeline(steps=[\n",
        "    ('feature_engineering', SelectableFeatureEngineeringTransformer(include_features=base_config['features'])),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(random_state=42)) # Model to tune\n",
        "])\n",
        "\n",
        "\n",
        "# C. Tentukan Grid Hyperparameter untuk GridSearchCV\n",
        "# Sesuaikan grid ini berdasarkan parameter yang ingin Anda uji untuk GradientBoostingRegressor\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [100, 200, 300],\n",
        "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'regressor__max_depth': [3, 5, 7],\n",
        "    'regressor__subsample': [0.8, 0.9, 1.0],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# --- 6. Hyperparameter Tuning dengan GridSearchCV ---\n",
        "print(\"Memulai Hyperparameter Tuning dengan GridSearchCV...\")\n",
        "\n",
        "# Gunakan KFold Cross-Validation\n",
        "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline_to_tune,\n",
        "    param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='neg_mean_absolute_error', # Menggunakan MAE sebagai metrik\n",
        "    n_jobs=-1, # Gunakan semua core CPU\n",
        "    verbose=2 # Tampilkan detail proses\n",
        ")\n",
        "\n",
        "# Lakukan tuning pada SELURUH data training yang sudah dibersihkan targetnya\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Tuning GridSearchCV selesai.\")\n",
        "print(f\"MAE terbaik dari CV: {-grid_search.best_score_:.4f}\") # Negate score to get positive MAE\n",
        "print(\"Parameter terbaik:\", grid_search.best_params_)\n",
        "\n",
        "# --- 7. Melatih Model Final & Membuat Prediksi ---\n",
        "print(\"\\nMelatih model final dengan parameter terbaik...\")\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Latih model terbaik pada SELURUH data training\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Membuat prediksi pada data test...\")\n",
        "final_predictions = best_model.predict(X_test)\n",
        "\n",
        "# --- 8. Membuat File Submission ---\n",
        "submission_df = pd.DataFrame({'id': X_test['id'], 'dropout_rate_percent': final_predictions})\n",
        "submission_df.to_csv('submission_gridsearch_tuned.csv', index=False)\n",
        "\n",
        "print(\"\\n========================================================\")\n",
        "print(\"File 'submission_gridsearch_tuned.csv' berhasil dibuat dari model hasil tuning GridSearchCV.\")\n",
        "print(\"========================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data berhasil dimuat.\n",
            "Memulai Hyperparameter Tuning dengan GridSearchCV...\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Tuning GridSearchCV selesai.\n",
            "MAE terbaik dari CV: 3.6587\n",
            "Parameter terbaik: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__min_samples_leaf': 2, 'regressor__n_estimators': 100, 'regressor__subsample': 0.8}\n",
            "\n",
            "Melatih model final dengan parameter terbaik...\n",
            "Membuat prediksi pada data test...\n",
            "\n",
            "========================================================\n",
            "File 'submission_gridsearch_tuned.csv' berhasil dibuat dari model hasil tuning GridSearchCV.\n",
            "========================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Grid Search CV 1000 estimators ---\n",
        "# !pip install optuna -q # Optuna not needed for this cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "# import optuna # Not needed\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
        "\n",
        "# --- 1. Setup Awal ---\n",
        "warnings.filterwarnings('ignore')\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING) # Not needed\n",
        "\n",
        "# --- 2. Memuat Data ---\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    print(\"Data berhasil dimuat.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Gagal memuat file. Detail: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Persiapan Data ---\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy()\n",
        "categorical_features_pre_fe = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# --- 4. Definisi Feature Engineering Transformer (Tetap Sama) ---\n",
        "class SelectableFeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, include_features=None):\n",
        "        self.include_features = include_features if include_features is not None else []\n",
        "    def fit(self, X, y=None): return self\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        if 'funding_per_teacher' in self.include_features: df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        if 'low_income_minority_interaction' in self.include_features: df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        if 'score_to_funding_ratio' in self.include_features: df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1e-6)\n",
        "        if 'internet_to_income_ratio' in self.include_features: df['internet_access_percent'] / (df['percent_low_income'] + 1e-6) # Fixed potential division by zero\n",
        "        if 'is_high_school' in self.include_features: df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        if 'is_middle_school' in self.include_features: df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        if 'is_elementary_school' in self.include_features: df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "        if 'teacher_load' in self.include_features: df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1 + 1e-6) # Fixed potential division by zero\n",
        "        if 'adjusted_funding' in self.include_features: df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "        if 'minority_to_internet_gap' in self.include_features: df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "        return df\n",
        "\n",
        "# --- 5. Mendefinisikan Pipeline dan Grid untuk Tuning ---\n",
        "\n",
        "# A. Konfigurasi Terbaik dari hasil Brute Force (sesuaikan jika hasil brute force Anda berbeda)\n",
        "# Menggunakan konfigurasi terbaik yang sering muncul: GradientBoosting, MeanImputer, MinMaxScaler, dengan fitur spesifik\n",
        "base_config = {\n",
        "    'features': ['is_high_school', 'is_middle_school', 'is_elementary_school', 'funding_per_teacher', 'low_income_minority_interaction', 'score_to_funding_ratio', 'minority_to_internet_gap'],\n",
        "    'imputer': SimpleImputer(strategy='mean'),\n",
        "    'scaler': MinMaxScaler(),\n",
        "}\n",
        "\n",
        "# B. Buat pipeline dasar dengan konfigurasi terbaik\n",
        "# Dapatkan daftar kolom numerik setelah FE untuk konfigurasi dasar\n",
        "temp_transformer = SelectableFeatureEngineeringTransformer(include_features=base_config['features'])\n",
        "temp_df = temp_transformer.transform(X_train.head())\n",
        "numerical_features_after_fe = temp_df.select_dtypes(include=np.number).columns.drop('id', errors='ignore').tolist()\n",
        "categorical_features_to_encode = [col for col in categorical_features_pre_fe if col != 'school_name']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[('imputer', base_config['imputer']), ('scaler', base_config['scaler'])])\n",
        "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # Assuming OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features_after_fe),\n",
        "        ('cat', categorical_transformer, categorical_features_to_encode)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Definisikan pipeline final untuk tuning\n",
        "pipeline_to_tune = Pipeline(steps=[\n",
        "    ('feature_engineering', SelectableFeatureEngineeringTransformer(include_features=base_config['features'])),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(random_state=42)) # Model to tune\n",
        "])\n",
        "\n",
        "\n",
        "# C. Tentukan Grid Hyperparameter untuk GridSearchCV\n",
        "# Sesuaikan grid ini berdasarkan parameter yang ingin Anda uji untuk GradientBoostingRegressor\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [1000],\n",
        "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'regressor__max_depth': [3, 5, 7],\n",
        "    'regressor__subsample': [0.8, 0.9, 1.0],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# --- 6. Hyperparameter Tuning dengan GridSearchCV ---\n",
        "print(\"Memulai Hyperparameter Tuning dengan GridSearchCV...\")\n",
        "\n",
        "# Gunakan KFold Cross-Validation\n",
        "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline_to_tune,\n",
        "    param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='neg_mean_absolute_error', # Menggunakan MAE sebagai metrik\n",
        "    n_jobs=-1, # Gunakan semua core CPU\n",
        "    verbose=2 # Tampilkan detail proses\n",
        ")\n",
        "\n",
        "# Lakukan tuning pada SELURUH data training yang sudah dibersihkan targetnya\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Tuning GridSearchCV selesai.\")\n",
        "print(f\"MAE terbaik dari CV: {-grid_search.best_score_:.4f}\") # Negate score to get positive MAE\n",
        "print(\"Parameter terbaik:\", grid_search.best_params_)\n",
        "\n",
        "# --- 7. Melatih Model Final & Membuat Prediksi ---\n",
        "print(\"\\nMelatih model final dengan parameter terbaik...\")\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Latih model terbaik pada SELURUH data training\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Membuat prediksi pada data test...\")\n",
        "final_predictions = best_model.predict(X_test)\n",
        "\n",
        "# --- 8. Membuat File Submission ---\n",
        "submission_df = pd.DataFrame({'id': X_test['id'], 'dropout_rate_percent': final_predictions})\n",
        "submission_df.to_csv('submission_gridsearch1k_tuned.csv', index=False)\n",
        "\n",
        "print(\"\\n========================================================\")\n",
        "print(\"File 'submission_gridsearch1k_tuned.csv' berhasil dibuat dari model hasil tuning GridSearchCV.\")\n",
        "print(\"========================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtveVykHZOeW",
        "outputId": "707038db-296b-4082-ad90-d8f5e0bc4724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data berhasil dimuat.\n",
            "Memulai Hyperparameter Tuning dengan GridSearchCV...\n",
            "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
            "Tuning GridSearchCV selesai.\n",
            "MAE terbaik dari CV: 3.7977\n",
            "Parameter terbaik: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__min_samples_leaf': 1, 'regressor__n_estimators': 1000, 'regressor__subsample': 0.8}\n",
            "\n",
            "Melatih model final dengan parameter terbaik...\n",
            "Membuat prediksi pada data test...\n",
            "\n",
            "========================================================\n",
            "File 'submission_gridsearch1k_tuned.csv' berhasil dibuat dari model hasil tuning GridSearchCV.\n",
            "========================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Grid Search CV (XGBoost vs LightGBM) + Sequential Feature Selector (SFS) (GPU Enabled) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import itertools # Need itertools for correct combination calculation\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import GradientBoostingRegressor # Keep import but won't be used in final pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, MaxAbsScaler, QuantileTransformer, PowerTransformer # Added Scalers\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector # Added make_column_selector\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
        "from sklearn.feature_selection import SequentialFeatureSelector # Import SFS\n",
        "\n",
        "# Import GPU-enabled models and DMatrix\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb # LightGBM also has GPU support\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Definisi Feature Engineering Transformer (Semua Fitur Potensial) ---\n",
        "# This transformer creates ALL potential FE features.\n",
        "class AllFeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        # --- ALL Feature Engineering steps ---\n",
        "        df['funding_per_teacher'] = df['funding_per_student_usd'] * df['student_teacher_ratio']\n",
        "        df['low_income_minority_interaction'] = df['percent_low_income'] * df['percent_minority']\n",
        "        df['score_to_funding_ratio'] = df['avg_test_score_percent'] / (df['funding_per_student_usd'] + 1e-6)\n",
        "        df['internet_to_income_ratio'] = df['internet_access_percent'] / (df['percent_low_income'] + 1e-6)\n",
        "        df['is_high_school'] = df['school_name'].str.contains('High', case=False, na=False).astype(int)\n",
        "        df['is_middle_school'] = df['school_name'].str.contains('Middle', case=False, na=False).astype(int)\n",
        "        df['is_elementary_school'] = df['school_name'].str.contains('Elementary', case=False, na=False).astype(int)\n",
        "        df['teacher_load'] = 1 / (df['student_teacher_ratio'] + 1 + 1e-6)\n",
        "        df['adjusted_funding'] = df['funding_per_student_usd'] * (df['internet_access_percent'] / 100)\n",
        "        df['minority_to_internet_gap'] = df['percent_minority'] - df['internet_access_percent']\n",
        "\n",
        "        return df\n",
        "\n",
        "# --- 1. Memuat Data ---\n",
        "try:\n",
        "    # Assuming data path is still '/kaggle/input/tes-binus/' as per user's previous input\n",
        "    data_path = '/kaggle/input/tes-binus/'\n",
        "    train_df = pd.read_csv(data_path + 'train_dataset.csv')\n",
        "    test_df = pd.read_csv(data_path + 'test_dataset.csv')\n",
        "    print(f\"Data berhasil dimuat dari {data_path}.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Gagal memuat file dari {data_path}. Detail: {e}\")\n",
        "    # Fallback to local path if Kaggle path fails\n",
        "    try:\n",
        "        train_df = pd.read_csv('train_dataset.csv')\n",
        "        test_df = pd.read_csv('test_dataset.csv')\n",
        "        print(\"Data berhasil dimuat dari local path.\")\n",
        "    except FileNotFoundError as e_local:\n",
        "        print(f\"Error: Gagal memuat file dari local path juga. Detail: {e_local}\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "# Prepare training and test data\n",
        "train_df.dropna(subset=['dropout_rate_percent'], inplace=True)\n",
        "X_train = train_df.drop('dropout_rate_percent', axis=1)\n",
        "y_train = train_df['dropout_rate_percent']\n",
        "X_test = test_df.copy() # Keep original for final prediction\n",
        "\n",
        "# Ensure school_name is string type before FE\n",
        "X_train[['school_name']] = X_train[['school_name']].astype(str)\n",
        "X_test[['school_name']] = X_test[['school_name']].astype(str)\n",
        "\n",
        "\n",
        "# --- 2. Apply ALL Potential Feature Engineering ---\n",
        "print(\"\\nMenambahkan semua fitur hasil Feature Engineering potensial...\")\n",
        "# Apply the transformer that creates all potential FE features\n",
        "fe_transformer_all = AllFeatureEngineeringTransformer()\n",
        "X_train_fe_all = fe_transformer_all.fit_transform(X_train)\n",
        "X_test_fe_all = fe_transformer_all.transform(X_test)\n",
        "\n",
        "print(f\"Jumlah fitur setelah menambahkan semua fitur FE potensial: {X_train_fe_all.shape[1]}\")\n",
        "\n",
        "\n",
        "# --- 3. Define Preprocessing steps (Imputation and Encoding) BEFORE SFS ---\n",
        "# This preprocessor will handle imputation and encoding for SFS input.\n",
        "imputer_step_pre_sfs_num = SimpleImputer(strategy='mean') # Imputer for numerical features\n",
        "imputer_step_pre_sfs_cat = SimpleImputer(strategy='most_frequent') # Imputer for categorical features\n",
        "encoder_step_pre_sfs = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # Encoder for categorical features\n",
        "\n",
        "# Identify numerical and categorical features AFTER applying ALL FE\n",
        "# These are the features that will be fed into this pre-SFS preprocessor\n",
        "numerical_features_all_fe = make_column_selector(dtype_include=np.number)(X_train_fe_all)\n",
        "# Exclude 'id' from numerical features if present\n",
        "numerical_features_all_fe = [col for col in numerical_features_all_fe if col not in ['id']]\n",
        "\n",
        "categorical_features_all_fe = make_column_selector(dtype_exclude=np.number)(X_train_fe_all)\n",
        "# Include 'school_name' here if it's a string column that needs encoding\n",
        "# If school_name is already used in FE to create numerical features, it might not be in dtype_exclude,\n",
        "# but let's explicitly handle it if it is still a string/object column.\n",
        "# Assuming 'school_name', 'state', 'school_type', 'grade_level' are the original categorical columns\n",
        "# and any new categorical features created by FE (like the is_school_type ones) are already numerical (int).\n",
        "original_categorical_features = ['school_name', 'state', 'school_type', 'grade_level'] # Assuming these are the original ones\n",
        "# Filter to only include those that are still in X_train_fe_all and are of object/string type\n",
        "categorical_features_to_encode_pre_sfs = [col for col in original_categorical_features if col in X_train_fe_all.columns and X_train_fe_all[col].dtype == 'object']\n",
        "\n",
        "\n",
        "preprocessor_pre_sfs = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[('imputer', imputer_step_pre_sfs_num)]), numerical_features_all_fe), # Only impute numerical for now\n",
        "        ('cat', Pipeline(steps=[('imputer', imputer_step_pre_sfs_cat), ('encoder', encoder_step_pre_sfs)]), categorical_features_to_encode_pre_sfs) # Impute and encode categorical\n",
        "    ],\n",
        "    remainder='passthrough' # Pass through features not explicitly handled (like the new numerical FE features)\n",
        ")\n",
        "\n",
        "# Apply pre-SFS preprocessing to the data\n",
        "print(\"\\nMenerapkan preprocessing (Imputasi & Encoding Kategorikal) sebelum SFS...\")\n",
        "X_train_processed_pre_sfs = preprocessor_pre_sfs.fit_transform(X_train_fe_all, y_train)\n",
        "\n",
        "\n",
        "# Revised Preprocessing Pipeline steps BEFORE SFS:\n",
        "# 1. Impute and Encode Categoricals (using OneHotEncoder)\n",
        "# 2. Impute Numerical (using MeanImputer)\n",
        "# 3. Combine numerical and encoded categorical features\n",
        "# 4. Scale All Numerical Features\n",
        "\n",
        "# Identify numerical and categorical features AFTER applying ALL FE\n",
        "numerical_features_all_fe = make_column_selector(dtype_include=np.number)(X_train_fe_all)\n",
        "numerical_features_all_fe = [col for col in numerical_features_all_fe if col not in ['id']] # Exclude 'id'\n",
        "\n",
        "categorical_features_all_fe = make_column_selector(dtype_exclude=np.number)(X_train_fe_all)\n",
        "# Assume these are the columns that need encoding (state, school_type, grade_level, school_name if still object)\n",
        "# Let's be explicit based on original data:\n",
        "original_categorical_features = ['school_name', 'state', 'school_type', 'grade_level']\n",
        "# Filter to only include those that are still object/string type after FE\n",
        "categorical_features_to_encode_pre_sfs = [col for col in original_categorical_features if col in X_train_fe_all.columns and X_train_fe_all[col].dtype == 'object']\n",
        "# The numerical FE features like is_high_school are already numerical and don't need encoding\n",
        "\n",
        "\n",
        "# Define preprocessor that encodes categoricals and passes through numericals\n",
        "preprocessor_encode_cat = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat_encode', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_features_to_encode_pre_sfs),\n",
        "        ('num_passthrough', 'passthrough', numerical_features_all_fe) # Pass through numerical features\n",
        "    ],\n",
        "    remainder='drop' # Drop any other columns (like 'id' or original categorical if not in list)\n",
        ")\n",
        "\n",
        "# Apply the encoding step\n",
        "print(\"\\nMenerapkan Encoding Kategorikal sebelum SFS...\")\n",
        "X_train_encoded = preprocessor_encode_cat.fit_transform(X_train_fe_all)\n",
        "\n",
        "# Now, X_train_encoded is a numpy array. We need to impute and scale.\n",
        "# Impute all numerical features (original numerical + new FE numerical + encoded categorical)\n",
        "imputer_all_num = SimpleImputer(strategy='mean') # Use mean imputer for simplicity before SFS\n",
        "X_train_imputed = imputer_all_num.fit_transform(X_train_encoded)\n",
        "\n",
        "# Scale all numerical features\n",
        "scaler_all_num = MinMaxScaler() # Use MinMaxScaler for simplicity before SFS\n",
        "X_train_scaled_pre_sfs = scaler_all_num.fit_transform(X_train_imputed)\n",
        "\n",
        "\n",
        "# X_train_scaled_pre_sfs is the data that will be fed into SFS\n",
        "# It's a numpy array. SFS works directly on numpy arrays.\n",
        "\n",
        "\n",
        "# --- 4. Define Base Model for SFS (GPU Enabled) ---\n",
        "# Use a model that SFS will use to evaluate feature subsets. XGBoost with GPU support.\n",
        "# Use default parameters or parameters known to work reasonably well for evaluation speed.\n",
        "# The base model for SFS should work directly on the preprocessed numerical data.\n",
        "base_model_for_sfs = xgb.XGBRegressor(random_state=42, n_estimators=100, learning_rate=0.1, max_depth=3, device='cuda') # XGBoost with GPU (updated)\n",
        "\n",
        "# --- 5. Apply SequentialFeatureSelector (SFS) ---\n",
        "\n",
        "# Determine the number of features to select (n_features_to_select)\n",
        "# Set to 'auto' to select the number of features maximizing the CV score.\n",
        "n_features_to_select = 'auto' # Or a fixed number, e.g., 20\n",
        "\n",
        "# --- Forward Selection ---\n",
        "print(\"\\nMelakukan Forward Feature Selection (SFS)...\")\n",
        "# SFS fits directly on the preprocessed numerical data\n",
        "sfs_forward = SequentialFeatureSelector(\n",
        "    base_model_for_sfs, # The base model to evaluate feature subsets\n",
        "    n_features_to_select=n_features_to_select,\n",
        "    direction='forward',\n",
        "    scoring='neg_mean_absolute_error', # Use MAE\n",
        "    cv=5, # Cross-validation folds\n",
        "    n_jobs=-1 # Use all cores\n",
        ")\n",
        "\n",
        "# Fit SFS on the preprocessed numerical data\n",
        "sfs_forward.fit(X_train_scaled_pre_sfs, y_train)\n",
        "\n",
        "# Need to map the selected feature indices back to original/FE feature names\n",
        "# This is complex because of the OneHotEncoder and passthrough.\n",
        "# For simplicity, let's just get the indices and work with the numpy arrays after SFS.\n",
        "selected_feature_indices_forward = sfs_forward.get_support(indices=True)\n",
        "print(f\"Fitur terpilih oleh Forward SFS ({len(selected_feature_indices_forward)} fitur - berdasarkan indeks setelah preprocessing):\")\n",
        "print(selected_feature_indices_forward)\n",
        "# Note: Interpreting these indices w.r.t original/FE feature names is hard here.\n",
        "\n",
        "\n",
        "# --- Backward Selection ---\n",
        "print(\"\\nMelakukan Backward Feature Selection (SFS)...\")\n",
        "# SFS fits directly on the preprocessed numerical data\n",
        "sfs_backward = SequentialFeatureSelector(\n",
        "    base_model_for_sfs, # The base model to evaluate feature subsets\n",
        "    n_features_to_select=n_features_to_select,\n",
        "    direction='backward',\n",
        "    scoring='neg_mean_absolute_error', # Use MAE\n",
        "    cv=5, # Cross-validation folds\n",
        "    n_jobs=-1 # Use all cores\n",
        ")\n",
        "\n",
        "# Fit SFS on the preprocessed numerical data\n",
        "sfs_backward.fit(X_train_scaled_pre_sfs, y_train)\n",
        "\n",
        "selected_feature_indices_backward = sfs_backward.get_support(indices=True)\n",
        "print(f\"Fitur terpilih oleh Backward SFS ({len(selected_feature_indices_backward)} fitur - berdasarkan indeks setelah preprocessing):\")\n",
        "print(selected_feature_indices_backward)\n",
        "\n",
        "\n",
        "# --- Choose the best set of features (e.g., based on size or manual choice) ---\n",
        "# For simplicity, let's use the features selected by Backward SFS (indices) for the next step.\n",
        "selected_feature_indices_final = selected_feature_indices_backward\n",
        "print(f\"\\nMenggunakan {len(selected_feature_indices_final)} fitur terpilih (dari Backward SFS indices) untuk tuning GridSearchCV.\")\n",
        "\n",
        "\n",
        "# --- 6. Prepare Data with SFS-Selected Features for GridSearchCV ---\n",
        "# We need to select columns by index from the fully preprocessed data\n",
        "# Apply the same full preprocessor to the data\n",
        "print(\"\\nMenerapkan Preprocessing Lengkap (Encode, Impute, Scale) pada data setelah FE...\")\n",
        "# Redefine the full preprocessor to ensure it's fit on the full data\n",
        "# Identify numerical and categorical features AFTER applying ALL FE\n",
        "numerical_features_all_fe = make_column_selector(dtype_include=np.number)(X_train_fe_all)\n",
        "numerical_features_all_fe = [col for col in numerical_features_all_fe if col not in ['id']] # Exclude 'id'\n",
        "\n",
        "categorical_features_all_fe = make_column_selector(dtype_exclude=np.number)(X_train_fe_all)\n",
        "original_categorical_features = ['school_name', 'state', 'school_type', 'grade_level'] # Assuming these are the original ones\n",
        "categorical_features_to_encode = [col for col in original_categorical_features if col in X_train_fe_all.columns and X_train_fe_all[col].dtype == 'object']\n",
        "\n",
        "\n",
        "full_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_features_to_encode),\n",
        "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), ('scaler', MinMaxScaler())]), numerical_features_all_fe) # Impute and scale numericals\n",
        "    ],\n",
        "    remainder='drop' # Drop columns not explicitly handled\n",
        ")\n",
        "\n",
        "X_train_processed_full = full_preprocessor.fit_transform(X_train_fe_all, y_train)\n",
        "X_test_processed_full = full_preprocessor.transform(X_test_fe_all)\n",
        "\n",
        "\n",
        "# Select columns by index from the fully preprocessed data\n",
        "X_train_sfs_selected = X_train_processed_full[:, selected_feature_indices_final]\n",
        "X_test_sfs_selected = X_test_processed_full[:, selected_feature_indices_final]\n",
        "\n",
        "print(f\"Data training siap untuk GridSearchCV dengan {X_train_sfs_selected.shape[1]} fitur terpilih.\")\n",
        "\n",
        "\n",
        "# --- 7. Define Pipeline for GridSearchCV (using SFS-Selected Features) ---\n",
        "# Define the final pipeline to tune with GridSearchCV.\n",
        "# This pipeline takes the SFS-selected data (which is already preprocessed) as input.\n",
        "pipeline_to_tune = Pipeline(steps=[\n",
        "    # No selector needed here, as the data is already selected\n",
        "    ('regressor', 'passthrough') # Placeholder, will be replaced by models in param_grid\n",
        "])\n",
        "\n",
        "\n",
        "# --- 8. Tentukan Grid Hyperparameter & Models untuk GridSearchCV ---\n",
        "# Define the parameter grid for GridSearchCV, including different models.\n",
        "# We will tune hyperparameters for BOTH XGBoost and LightGBM.\n",
        "\n",
        "# Define models with their GPU parameters\n",
        "xgb_gpu = xgb.XGBRegressor(random_state=42, device='cuda') # XGBoost with GPU (updated)\n",
        "lgbm_gpu = lgb.LGBMRegressor(random_state=42, device='gpu') # Use 'gpu' for LightGBM GPU\n",
        "\n",
        "# Define parameter grids for each model\n",
        "param_grid_xgb = {\n",
        "    'regressor': [xgb_gpu], # Specify the model instance\n",
        "    'regressor__n_estimators': [100, 200, 300],\n",
        "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'regressor__max_depth': [3, 5, 7],\n",
        "    'regressor__subsample': [0.8, 0.9, 1.0],\n",
        "    'regressor__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "param_grid_lgbm = {\n",
        "    'regressor': [lgbm_gpu], # Specify the model instance\n",
        "    'regressor__n_estimators': [100, 200, 300],\n",
        "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'regressor__max_depth': [3, 5, 7], # LightGBM uses max_depth differently, but this is a common param\n",
        "    'regressor__subsample': [0.8, 0.9, 1.0],\n",
        "    'regressor__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Combine parameter grids\n",
        "# Note: GridSearchCV will test ALL combinations within each grid.\n",
        "# To test both models, we can create a list of parameter grids.\n",
        "param_grid = [\n",
        "    param_grid_xgb,\n",
        "    param_grid_lgbm,\n",
        "]\n",
        "\n",
        "# Reduce grid size for faster testing if needed\n",
        "param_grid_xgb_small = {\n",
        "    'regressor': [xgb_gpu],\n",
        "    'regressor__n_estimators': [100],\n",
        "    'regressor__learning_rate': [0.05],\n",
        "    'regressor__max_depth': [3],\n",
        "    'regressor__subsample': [0.8], # Added subsample\n",
        "    'regressor__colsample_bytree': [0.8] # Added colsample_bytree\n",
        "}\n",
        "param_grid_lgbm_small = {\n",
        "    'regressor': [lgbm_gpu],\n",
        "    'regressor__n_estimators': [100],\n",
        "    'regressor__learning_rate': [0.05],\n",
        "    'regressor__max_depth': [3],\n",
        "    'regressor__subsample': [0.8], # Added subsample\n",
        "    'regressor__colsample_bytree': [0.8] # Added colsample_bytree\n",
        "}\n",
        "\n",
        "param_grid = [\n",
        "    param_grid_xgb_small,\n",
        "    param_grid_lgbm_small,\n",
        "]\n",
        "\n",
        "\n",
        "# Calculate total combinations\n",
        "total_combinations = 0\n",
        "for grid in param_grid:\n",
        "    # Calculate combinations within each grid, excluding the 'regressor' key itself\n",
        "    grid_combinations = 1\n",
        "    for key, values in grid.items():\n",
        "        if key != 'regressor':\n",
        "            grid_combinations *= len(values)\n",
        "    # Add 1 for the model choice itself (either xgb_gpu or lgbm_gpu) - this is handled by the list of grids\n",
        "    total_combinations += grid_combinations\n",
        "\n",
        "print(f\"Jumlah total kombinasi hyperparameter dan model yang akan diuji oleh GridSearchCV: {total_combinations}\")\n",
        "\n",
        "\n",
        "# --- 9. Hyperparameter Tuning dengan GridSearchCV ---\n",
        "print(\"\\nMemulai Hyperparameter Tuning dengan GridSearchCV pada fitur terpilih oleh SFS (data preproses lengkap), menguji XGBoost dan LightGBM...\")\n",
        "\n",
        "# Use KFold Cross-Validation\n",
        "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline_to_tune,\n",
        "    param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='neg_mean_absolute_error', # Menggunakan MAE sebagai metrik\n",
        "    n_jobs=-1, # Gunakan semua core CPU\n",
        "    verbose=2 # Tampilkan detail proses\n",
        ")\n",
        "\n",
        "# Lakukan tuning pada data training *setelah* dipreproses lengkap DAN *setelah* SFS selection\n",
        "grid_search.fit(X_train_sfs_selected, y_train)\n",
        "\n",
        "print(\"\\nTuning GridSearchCV selesai.\")\n",
        "print(f\"MAE terbaik dari CV: {-grid_search.best_score_:.4f}\") # Negate score to get positive MAE\n",
        "print(\"Parameter terbaik:\", grid_search.best_params_)\n",
        "print(f\"Model terbaik: {type(grid_search.best_estimator_.named_steps['regressor']).__name__}\")\n",
        "\n",
        "\n",
        "# --- 10. Melatih Model Final & Membuat Prediksi ---\n",
        "print(\"\\nMelatih model final dengan parameter terbaik pada fitur terpilih...\")\n",
        "# The best_estimator_ from GridSearchCV is the pipeline with the best model and parameters.\n",
        "best_model_sfs_tuned = grid_search.best_estimator_\n",
        "\n",
        "# The best_model_sfs_tuned is already trained by grid_search.fit.\n",
        "# We just need to use it for prediction.\n",
        "\n",
        "\n",
        "print(\"Membuat prediksi pada data test menggunakan fitur terpilih...\")\n",
        "\n",
        "# Apply the same full preprocessor to the test data\n",
        "# X_test_processed_full was already created after applying full FE to X_test\n",
        "# Select the SFS-selected features from the fully preprocessed test data\n",
        "X_test_sfs_selected = X_test_processed_full[:, selected_feature_indices_final]\n",
        "\n",
        "\n",
        "# --- Fix for XGBoost GPU prediction warning ---\n",
        "# Check if the best model is XGBoost and move test data to DMatrix on GPU\n",
        "if isinstance(best_model_sfs_tuned.named_steps['regressor'], xgb.XGBRegressor):\n",
        "    print(\"Menggunakan XGBoost untuk prediksi. Mengonversi data test ke DMatrix pada GPU...\")\n",
        "    # Access the trained XGBoost model within the pipeline\n",
        "    trained_xgb_model = best_model_sfs_tuned.named_steps['regressor']\n",
        "    # Convert the selected test data to DMatrix on the correct device\n",
        "    dtest = xgb.DMatrix(X_test_sfs_selected, enable_categorical=False) # Assuming data is numerical/encoded\n",
        "    # Ensure DMatrix is on the same device as the model if needed, though device='cuda' should handle it\n",
        "    # dtest.set_device(trained_xgb_model.get_params()['device']) # Not needed with device='cuda' in DMatrix\n",
        "\n",
        "    final_predictions_sfs_tuned = trained_xgb_model.predict(dtest)\n",
        "    print(\"Prediksi XGBoost selesai.\")\n",
        "\n",
        "elif isinstance(best_model_sfs_tuned.named_steps['regressor'], lgb.LGBMRegressor):\n",
        "     print(\"Menggunakan LightGBM untuk prediksi.\")\n",
        "     # LightGBM's predict method often handles data transfer internally when device='gpu' is set.\n",
        "     final_predictions_sfs_tuned = best_model_sfs_tuned.predict(X_test_sfs_selected)\n",
        "     print(\"Prediksi LightGBM selesai.\")\n",
        "\n",
        "else:\n",
        "     print(\"Menggunakan model non-GPU untuk prediksi.\")\n",
        "     final_predictions_sfs_tuned = best_model_sfs_tuned.predict(X_test_sfs_selected)\n",
        "     print(\"Prediksi model non-GPU selesai.\")\n",
        "\n",
        "\n",
        "# --- 11. Membuat File Submission ---\n",
        "submission_sfs_tuned_df = pd.DataFrame({'id': X_test['id'], 'dropout_rate_percent': final_predictions_sfs_tuned}) # Use original test IDs\n",
        "submission_sfs_tuned_df.to_csv('submission_sfs_gridsearch_tuned_xgb_lgbm.csv', index=False) # Unique file name\n",
        "print(\"\\n========================================================\")\n",
        "print(\"File 'submission_sfs_gridsearch_tuned_xgb_lgbm.csv' berhasil dibuat dari model hasil SFS + tuning GridSearchCV (XGB/LGBM).\")\n",
        "print(\"========================================================\")\n",
        "\n",
        "# --- 12. Evaluate Final Model ---\n",
        "try:\n",
        "    # Load the true values for evaluation\n",
        "    kunjaw_df = pd.read_csv('/kaggle/input/tes-binus/KunJaw Predicted.csv')\n",
        "    y_true = kunjaw_df['dropout_rate_percent']\n",
        "    print(\"\\nFile 'KunJaw Predicted.csv' berhasil dimuat untuk evaluasi final.\")\n",
        "\n",
        "    # Evaluate the final predictions\n",
        "    if len(y_true) == len(final_predictions_sfs_tuned):\n",
        "        final_mae_sfs_tuned = mean_absolute_error(y_true, final_predictions_sfs_tuned)\n",
        "        print(f\"Mean Absolute Error (MAE) Model Final SFS + GridSearchCV (XGB/LGBM) vs KunJaw: {final_mae_sfs_tuned:.4f}\")\n",
        "    else:\n",
        "        print(f\"Warning: Panjang prediksi final SFS + GridSearchCV (XGB/LGBM) ({len(final_predictions_sfs_tuned)}) tidak sesuai dengan KunJaw ({len(y_true)}). Tidak dapat menghitung MAE.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nError: File 'KunJaw Predicted.csv' tidak ditemukan. Tidak dapat melakukan evaluasi MAE final.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "MLd5N0bKw9OM",
        "outputId": "0a8b1c5c-c493-42fc-a921-d4786841c983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data berhasil dimuat.\n",
            "\n",
            "Menambahkan semua fitur hasil Feature Engineering potensial...\n",
            "Jumlah fitur setelah menambahkan semua fitur FE potensial: 20\n",
            "\n",
            "Menerapkan preprocessing (Imputasi & Encoding Kategorikal) sebelum SFS...\n",
            "\n",
            "Menerapkan Encoding Kategorikal sebelum SFS...\n",
            "\n",
            "Melakukan Forward Feature Selection (SFS)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1432149561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;31m# Fit SFS on the preprocessed numerical data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m \u001b[0msfs_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled_pre_sfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;31m# Need to map the selected feature indices back to original/FE feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_selection/_sequential.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mprocess_routing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             new_feature_idx, new_score = self._get_best_new_feature_score(\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0mcloned_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_selection/_sequential.py\u001b[0m in \u001b[0;36m_get_best_new_feature_score\u001b[0;34m(self, estimator, X, y, cv, current_mask, **params)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mcandidate_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mcandidate_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             scores[feature_idx] = cross_val_score(\n\u001b[0m\u001b[1;32m    314\u001b[0m                 \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac5274554a9e4d8ca713372a322e6dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_203948c8f2a749e5b5fbaa85dd027441",
              "IPY_MODEL_c48cd55aad3c4f07b34f4a76dd5bb80f",
              "IPY_MODEL_f4f812c2f69c4157a183bdf17db4050c"
            ],
            "layout": "IPY_MODEL_26ebe28791484a98ac5c6e3edbd7d745"
          }
        },
        "203948c8f2a749e5b5fbaa85dd027441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37435b1059a2409d9493eda905117810",
            "placeholder": "",
            "style": "IPY_MODEL_4349e1c5aca54259af72edb2fb8cbbab",
            "value": "MencariPipelineTerbaik:100%"
          }
        },
        "c48cd55aad3c4f07b34f4a76dd5bb80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4565c292997047e4ba93ec347df1015a",
            "max": 240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e9d0bff89944f0da882d748b0a04a45",
            "value": 240
          }
        },
        "f4f812c2f69c4157a183bdf17db4050c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be88b2012e3e41928da3226d4918000d",
            "placeholder": "",
            "style": "IPY_MODEL_ee0fcec9560b4bf2953de5a0b9132ecf",
            "value": "240/240[02:03&lt;00:00,3.87it/s]"
          }
        },
        "26ebe28791484a98ac5c6e3edbd7d745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37435b1059a2409d9493eda905117810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4349e1c5aca54259af72edb2fb8cbbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4565c292997047e4ba93ec347df1015a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e9d0bff89944f0da882d748b0a04a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be88b2012e3e41928da3226d4918000d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0fcec9560b4bf2953de5a0b9132ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f4de59f4a04bb598b108dfff1a0796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be2a3e4b444d4f139bc62ab6e87b9899",
              "IPY_MODEL_83237122b90e47849e03b37cfabd46ee",
              "IPY_MODEL_af1b2e6bfe0e4224b12ee47d48517023"
            ],
            "layout": "IPY_MODEL_ab87e13dc529437997ec7ddc5a02863b"
          }
        },
        "be2a3e4b444d4f139bc62ab6e87b9899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847072a21a624765bb18ad7fec52c3df",
            "placeholder": "",
            "style": "IPY_MODEL_01186afde6a647fcbc3b3975937a33dd",
            "value": "MencariPipelineTerbaik:100%"
          }
        },
        "83237122b90e47849e03b37cfabd46ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f9fa4a0e0ca4388ac1c0660e8b23abb",
            "max": 1536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37bed47d2a414ae0a802b8efaad9686b",
            "value": 1536
          }
        },
        "af1b2e6bfe0e4224b12ee47d48517023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8ffedb037224624952425441f07c80c",
            "placeholder": "",
            "style": "IPY_MODEL_1fbe2358245940829da5933d2274066c",
            "value": "1536/1536[08:31&lt;00:00,1.70it/s]"
          }
        },
        "ab87e13dc529437997ec7ddc5a02863b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847072a21a624765bb18ad7fec52c3df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01186afde6a647fcbc3b3975937a33dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f9fa4a0e0ca4388ac1c0660e8b23abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37bed47d2a414ae0a802b8efaad9686b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8ffedb037224624952425441f07c80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fbe2358245940829da5933d2274066c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "501356eacd214486aa9a5fdac779f9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71536175337441469c2b3260b30e409e",
              "IPY_MODEL_df2fcedab98b47b993dac525a1e92415",
              "IPY_MODEL_6f10c341bc574fda911e6a97574c5754"
            ],
            "layout": "IPY_MODEL_9d3aa81e52a249cc8ed8eaaed06e9b11"
          }
        },
        "71536175337441469c2b3260b30e409e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04686774d12244cab5d60b394845f4c3",
            "placeholder": "",
            "style": "IPY_MODEL_3ea26ab3d0964d45902d5fd76db9cbc5",
            "value": "Besttrial:82.Bestvalue:3.66608:100%"
          }
        },
        "df2fcedab98b47b993dac525a1e92415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fe26a3a178a490da28d4f55e08b72ac",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b4e4498adc3466da37e6d692b206664",
            "value": 100
          }
        },
        "6f10c341bc574fda911e6a97574c5754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4783af79176d4e4b91f6fed87a6ac1e8",
            "placeholder": "",
            "style": "IPY_MODEL_48f803bcea8b454b8e6d72a38f9727dc",
            "value": "100/100[16:12&lt;00:00,2.18s/it]"
          }
        },
        "9d3aa81e52a249cc8ed8eaaed06e9b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04686774d12244cab5d60b394845f4c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea26ab3d0964d45902d5fd76db9cbc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fe26a3a178a490da28d4f55e08b72ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b4e4498adc3466da37e6d692b206664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4783af79176d4e4b91f6fed87a6ac1e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f803bcea8b454b8e6d72a38f9727dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}